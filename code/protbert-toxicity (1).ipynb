{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8732319,"sourceType":"datasetVersion","datasetId":5241556},{"sourceId":8732333,"sourceType":"datasetVersion","datasetId":5241568},{"sourceId":9088340,"sourceType":"datasetVersion","datasetId":5483951},{"sourceId":9346585,"sourceType":"datasetVersion","datasetId":5665031}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers pandas scikit-learn torch lazypredict\n","metadata":{"_uuid":"9fa324ca-9f63-4a0e-9b40-c263d9b4d023","_cell_guid":"976fc78c-7561-46a4-89a9-301a83a414ac","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-25T18:10:53.143942Z","iopub.execute_input":"2024-09-25T18:10:53.144401Z","iopub.status.idle":"2024-09-25T18:11:07.719062Z","shell.execute_reply.started":"2024-09-25T18:10:53.144352Z","shell.execute_reply":"2024-09-25T18:11:07.717963Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nCollecting lazypredict\n  Downloading lazypredict-0.2.12-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from lazypredict) (8.1.7)\nRequirement already satisfied: lightgbm in /opt/conda/lib/python3.10/site-packages (from lazypredict) (4.2.0)\nRequirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (from lazypredict) (2.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\nInstalling collected packages: lazypredict\nSuccessfully installed lazypredict-0.2.12\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:11:07.720858Z","iopub.execute_input":"2024-09-25T18:11:07.721177Z","iopub.status.idle":"2024-09-25T18:11:08.087283Z","shell.execute_reply.started":"2024-09-25T18:11:07.721147Z","shell.execute_reply":"2024-09-25T18:11:08.086551Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## peptides","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertForMaskedLM, BertTokenizer, BertModel\n\n# Load the CSV file\n# file_path = 'path/to/your/combined_peptides.csv'\ndata = pd.read_csv('/kaggle/input/new-combined-data/combined_peptides.csv')\n\n# Load the pretrained ProtBERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert', do_lower_case=False)\nmodel = BertModel.from_pretrained('Rostlab/prot_bert')\nmodel.to('cuda')\n\ndef add_spaces(s):\n    return ' '.join(s)\n\n# Prepare the sequence data\nsequences = data['sequence'].tolist()\nsequences = [add_spaces(s) for s in sequences]\nlabels = data['label'].tolist()\n\n# Tokenize sequences\ndef tokenize_sequences(sequences, tokenizer, max_length=512):\n    tokenized_sequences = []\n    for seq in sequences:\n        # Tokenize and encode the sequence\n        inputs = tokenizer(seq, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n        tokenized_sequences.append(inputs)\n    return tokenized_sequences\n\ntokenized_sequences = tokenize_sequences(sequences, tokenizer)\n\n# Extract features\n# Extract features\ndef extract_features(tokenized_sequences, model):\n    model.eval()\n    all_embeddings = []\n    with torch.no_grad():\n        for tokenized_sequence in tokenized_sequences:\n            # Move tokenized sequence to CUDA\n            tokenized_sequence = {key: val.to('cuda') for key, val in tokenized_sequence.items()}\n            # Get model outputs\n            outputs = model(**tokenized_sequence)\n            # Compute mean pooling on the token embeddings\n            sequence_embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n            all_embeddings.append(sequence_embeddings)\n    return all_embeddings\n\nsequence_embeddings = extract_features(tokenized_sequences, model)\n\n# Convert the embeddings to a DataFrame\nembedding_df = pd.DataFrame(sequence_embeddings)\nembedding_df['label'] = labels\n\nembedding_df.to_csv('/kaggle/working/embeddings.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:11:08.088365Z","iopub.execute_input":"2024-09-25T18:11:08.088719Z","iopub.status.idle":"2024-09-25T18:20:09.976100Z","shell.execute_reply.started":"2024-09-25T18:11:08.088694Z","shell.execute_reply":"2024-09-25T18:20:09.975288Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/86.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a73872edfc814b7c84969c12d90554a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0885f2fb4a446d2b6b92696a31feeb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72046367253f47df857bbfaec2de81c1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/361 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522d8cec40d44b2f9fc073484ed5ab43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.68G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e21bb2102cf340efbbd5e6e48c5ffebf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load the embeddings\nembedding_df = pd.read_csv('/kaggle/working/embeddings.csv')\n\n# Split the data into features and labels\nX = embedding_df.drop(columns=['label'])\ny = embedding_df['label']\n\n# First split into training and temporary (test + validation) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Second split to divide temporary set into validation and test sets\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Save the split datasets\nX_train.to_csv('/kaggle/working/X_train.csv', index=False)\nX_test.to_csv('/kaggle/working/X_test.csv', index=False)\ny_train.to_csv('/kaggle/working/y_train.csv', index=False)\ny_test.to_csv('/kaggle/working/y_test.csv', index=False)\nX_val.to_csv('/kaggle/working/X_val.csv', index=False)\ny_val.to_csv('/kaggle/working/y_val.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:20:09.978159Z","iopub.execute_input":"2024-09-25T18:20:09.978483Z","iopub.status.idle":"2024-09-25T18:20:20.017917Z","shell.execute_reply.started":"2024-09-25T18:20:09.978458Z","shell.execute_reply":"2024-09-25T18:20:20.017135Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from lazypredict.Supervised import LazyClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the split datasets\nX_train = pd.read_csv('/kaggle/working/X_train.csv')\nX_test = pd.read_csv('/kaggle/working/X_test.csv')\nX_val = pd.read_csv('/kaggle/working/X_val.csv')\ny_train = pd.read_csv('/kaggle/working/y_train.csv').values.ravel()\ny_test = pd.read_csv('/kaggle/working/y_test.csv').values.ravel()\ny_val = pd.read_csv('/kaggle/working/y_val.csv').values.ravel()\n\n# Initialize LazyClassifier\nclf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n\n# Train and test the models\nmodels, predictions = clf.fit(X_train, X_val, y_train, y_val)\n\n# Display the results\nprint(models)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:20:20.018884Z","iopub.execute_input":"2024-09-25T18:20:20.019184Z","iopub.status.idle":"2024-09-25T18:22:34.719973Z","shell.execute_reply.started":"2024-09-25T18:20:20.019160Z","shell.execute_reply":"2024-09-25T18:22:34.719064Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":" 97%|█████████▋| 28/29 [01:58<00:03,  3.83s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 1812, number of negative: 1560\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035018 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 3372, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537367 -> initscore=0.149745\n[LightGBM] [Info] Start training from score 0.149745\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 29/29 [02:07<00:00,  4.41s/it]","output_type":"stream"},{"name":"stdout","text":"                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\nModel                                                                           \nQuadraticDiscriminantAnalysis      0.91               0.91     0.91      0.91   \nSVC                                0.89               0.89     0.89      0.89   \nLGBMClassifier                     0.89               0.89     0.89      0.89   \nRidgeClassifierCV                  0.88               0.88     0.88      0.88   \nXGBClassifier                      0.89               0.88     0.88      0.89   \nRidgeClassifier                    0.88               0.88     0.88      0.88   \nLinearDiscriminantAnalysis         0.88               0.88     0.88      0.88   \nExtraTreesClassifier               0.88               0.88     0.88      0.88   \nRandomForestClassifier             0.88               0.87     0.87      0.88   \nLogisticRegression                 0.87               0.87     0.87      0.87   \nCalibratedClassifierCV             0.87               0.87     0.87      0.87   \nBaggingClassifier                  0.86               0.86     0.86      0.86   \nPerceptron                         0.86               0.86     0.86      0.86   \nPassiveAggressiveClassifier        0.86               0.86     0.86      0.86   \nKNeighborsClassifier               0.86               0.86     0.86      0.86   \nSGDClassifier                      0.86               0.86     0.86      0.86   \nNuSVC                              0.85               0.85     0.85      0.85   \nLinearSVC                          0.85               0.85     0.85      0.85   \nAdaBoostClassifier                 0.83               0.83     0.83      0.83   \nDecisionTreeClassifier             0.82               0.82     0.82      0.82   \nExtraTreeClassifier                0.79               0.79     0.79      0.79   \nBernoulliNB                        0.72               0.71     0.71      0.72   \nGaussianNB                         0.72               0.71     0.71      0.72   \nNearestCentroid                    0.72               0.71     0.71      0.71   \nLabelPropagation                   0.64               0.66     0.66      0.59   \nLabelSpreading                     0.64               0.66     0.66      0.59   \nDummyClassifier                    0.54               0.50     0.50      0.38   \n\n                               Time Taken  \nModel                                      \nQuadraticDiscriminantAnalysis        2.27  \nSVC                                  4.09  \nLGBMClassifier                       9.61  \nRidgeClassifierCV                    1.00  \nXGBClassifier                       10.83  \nRidgeClassifier                      0.37  \nLinearDiscriminantAnalysis           1.19  \nExtraTreesClassifier                 1.61  \nRandomForestClassifier               8.54  \nLogisticRegression                   0.57  \nCalibratedClassifierCV              15.84  \nBaggingClassifier                   31.35  \nPerceptron                           0.55  \nPassiveAggressiveClassifier          0.99  \nKNeighborsClassifier                 0.42  \nSGDClassifier                        0.70  \nNuSVC                                5.31  \nLinearSVC                            4.03  \nAdaBoostClassifier                  21.00  \nDecisionTreeClassifier               4.79  \nExtraTreeClassifier                  0.17  \nBernoulliNB                          0.32  \nGaussianNB                           0.20  \nNearestCentroid                      0.26  \nLabelPropagation                     0.69  \nLabelSpreading                       0.86  \nDummyClassifier                      0.17  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc, confusion_matrix, classification_report\n\n# Define the model dictionary including AdaBoostClassifier\nmodel_dict = {\n    'LGBMClassifier': LGBMClassifier(),\n    'XGBClassifier': XGBClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'LogisticRegression': LogisticRegression(),\n    'SVC': SVC(probability=True),  # SVC needs probability=True for AUROC\n    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n}\n\n# Function to calculate additional metrics\ndef calculate_metrics(y_true, y_pred, y_prob=None):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    mcc = matthews_corrcoef(y_true, y_pred)\n    \n    sensitivity = tp / (tp + fn)  # Sensitivity (SN)\n    specificity = tn / (tn + fp)  # Specificity (SP)\n    fdr = fp / (fp + tp)  # False Discovery Rate (FDR)\n    \n    if y_prob is not None:\n        auroc = roc_auc_score(y_true, y_prob[:, 1])\n        precision, recall, _ = precision_recall_curve(y_true, y_prob[:, 1])\n        auprc = auc(recall, precision)\n    else:\n        auroc = None\n        auprc = None\n\n    return {\n        'accuracy': accuracy,\n        'f1_score': f1,\n        'mcc': mcc,\n        'sensitivity': sensitivity,\n        'specificity': specificity,\n        'fdr': fdr,\n        'auroc': auroc,\n        'auprc': auprc\n    }\n\n# Fit and evaluate each model\nbest_model_name = None\nbest_accuracy = 0\nresults = {}\n\nfor model_name, model in model_dict.items():\n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    val_predictions = model.predict(X_val)\n    test_predictions = model.predict(X_test)\n    \n    # Probability predictions for ROC and AUPRC\n    if hasattr(model, \"predict_proba\"):\n        val_prob = model.predict_proba(X_val)\n        test_prob = model.predict_proba(X_test)\n    else:\n        val_prob = None\n        test_prob = None\n    \n    # Calculate metrics for validation set\n    val_metrics = calculate_metrics(y_val, val_predictions, val_prob)\n    test_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n    \n    # Store results\n    results[model_name] = {\n        'validation_metrics': val_metrics,\n        'test_metrics': test_metrics\n    }\n    \n    # Print metrics\n    print(f\"\\n{model_name} Validation Metrics:\")\n    for metric, value in val_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    print(f\"\\n{model_name} Test Metrics:\")\n    for metric, value in test_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    # Track the best model by accuracy\n    if val_metrics['accuracy'] > best_accuracy:\n        best_accuracy = val_metrics['accuracy']\n        best_model_name = model_name\n\nprint(f\"\\nBest Model Name: {best_model_name}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:22:34.721242Z","iopub.execute_input":"2024-09-25T18:22:34.721532Z","iopub.status.idle":"2024-09-25T18:23:47.528730Z","shell.execute_reply.started":"2024-09-25T18:22:34.721507Z","shell.execute_reply":"2024-09-25T18:23:47.527560Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 1812, number of negative: 1560\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037674 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 3372, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537367 -> initscore=0.149745\n[LightGBM] [Info] Start training from score 0.149745\n\nLGBMClassifier Validation Metrics:\naccuracy: 0.8906\nf1_score: 0.9012\nmcc: 0.7805\nsensitivity: 0.9288\nspecificity: 0.8462\nfdr: 0.1248\nauroc: 0.9643\nauprc: 0.9667\n\nLGBMClassifier Test Metrics:\naccuracy: 0.8862\nf1_score: 0.8978\nmcc: 0.7700\nsensitivity: 0.9109\nspecificity: 0.8563\nfdr: 0.1150\nauroc: 0.9573\nauprc: 0.9618\n\nXGBClassifier Validation Metrics:\naccuracy: 0.8861\nf1_score: 0.8978\nmcc: 0.7720\nsensitivity: 0.9305\nspecificity: 0.8346\nfdr: 0.1327\nauroc: 0.9633\nauprc: 0.9694\n\nXGBClassifier Test Metrics:\naccuracy: 0.8933\nf1_score: 0.9046\nmcc: 0.7845\nsensitivity: 0.9222\nspecificity: 0.8583\nfdr: 0.1123\nauroc: 0.9602\nauprc: 0.9663\n\nRandomForestClassifier Validation Metrics:\naccuracy: 0.8852\nf1_score: 0.8967\nmcc: 0.7700\nsensitivity: 0.9272\nspecificity: 0.8365\nfdr: 0.1318\nauroc: 0.9586\nauprc: 0.9649\n\nRandomForestClassifier Test Metrics:\naccuracy: 0.8747\nf1_score: 0.8878\nmcc: 0.7466\nsensitivity: 0.9044\nspecificity: 0.8386\nfdr: 0.1281\nauroc: 0.9468\nauprc: 0.9538\n\nAdaBoostClassifier Validation Metrics:\naccuracy: 0.8301\nf1_score: 0.8433\nmcc: 0.6579\nsensitivity: 0.8510\nspecificity: 0.8058\nfdr: 0.1642\nauroc: 0.9089\nauprc: 0.9123\n\nAdaBoostClassifier Test Metrics:\naccuracy: 0.8240\nf1_score: 0.8408\nmcc: 0.6441\nsensitivity: 0.8476\nspecificity: 0.7953\nfdr: 0.1659\nauroc: 0.9056\nauprc: 0.9116\n\nLogisticRegression Validation Metrics:\naccuracy: 0.8488\nf1_score: 0.8622\nmcc: 0.6955\nsensitivity: 0.8808\nspecificity: 0.8115\nfdr: 0.1556\nauroc: 0.9282\nauprc: 0.9352\n\nLogisticRegression Test Metrics:\naccuracy: 0.8409\nf1_score: 0.8569\nmcc: 0.6781\nsensitivity: 0.8687\nspecificity: 0.8071\nfdr: 0.1546\nauroc: 0.9220\nauprc: 0.9317\n\nSVC Validation Metrics:\naccuracy: 0.8158\nf1_score: 0.8353\nmcc: 0.6296\nsensitivity: 0.8692\nspecificity: 0.7538\nfdr: 0.1960\nauroc: 0.9072\nauprc: 0.9200\n\nSVC Test Metrics:\naccuracy: 0.8276\nf1_score: 0.8468\nmcc: 0.6510\nsensitivity: 0.8687\nspecificity: 0.7776\nfdr: 0.1741\nauroc: 0.9047\nauprc: 0.9155\n\nQuadraticDiscriminantAnalysis Validation Metrics:\naccuracy: 0.9057\nf1_score: 0.9097\nmcc: 0.8127\nsensitivity: 0.8841\nspecificity: 0.9308\nfdr: 0.0632\nauroc: 0.9296\nauprc: 0.9498\n\nQuadraticDiscriminantAnalysis Test Metrics:\naccuracy: 0.9004\nf1_score: 0.9065\nmcc: 0.8019\nsensitivity: 0.8801\nspecificity: 0.9252\nfdr: 0.0654\nauroc: 0.9220\nauprc: 0.9463\n\nBest Model Name: QuadraticDiscriminantAnalysis\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:08:46.595101Z","iopub.execute_input":"2024-09-25T19:08:46.595933Z","iopub.status.idle":"2024-09-25T19:08:46.600039Z","shell.execute_reply.started":"2024-09-25T19:08:46.595899Z","shell.execute_reply":"2024-09-25T19:08:46.599188Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Define base models for stacking\nbase_models = [\n    ('qda', QuadraticDiscriminantAnalysis()),\n    ('svc', SVC(probability=True)),\n    ('lgbm', LGBMClassifier())\n]\n\n# Create stacking classifier\nstacking_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression(),  # You can choose any classifier here\n    cv=5  # Use 5-fold cross-validation\n)\n\n# Fit the stacking model\nstacking_model.fit(X_train, y_train)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:08:50.135450Z","iopub.execute_input":"2024-09-25T19:08:50.135819Z","iopub.status.idle":"2024-09-25T19:12:40.601321Z","shell.execute_reply.started":"2024-09-25T19:08:50.135788Z","shell.execute_reply":"2024-09-25T19:12:40.599685Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2733, number of negative: 4511\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.145838 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 7244, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377278 -> initscore=-0.501119\n[LightGBM] [Info] Start training from score -0.501119\n[LightGBM] [Info] Number of positive: 2187, number of negative: 3608\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200358 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377394 -> initscore=-0.500623\n[LightGBM] [Info] Start training from score -0.500623\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115497 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116423 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116885 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 2187, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137144 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5796, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377329 -> initscore=-0.500900\n[LightGBM] [Info] Start training from score -0.500900\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"StackingClassifier(cv=5,\n                   estimators=[('qda', QuadraticDiscriminantAnalysis()),\n                               ('svc', SVC(probability=True)),\n                               ('lgbm', LGBMClassifier())],\n                   final_estimator=LogisticRegression())","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(cv=5,\n                   estimators=[(&#x27;qda&#x27;, QuadraticDiscriminantAnalysis()),\n                               (&#x27;svc&#x27;, SVC(probability=True)),\n                               (&#x27;lgbm&#x27;, LGBMClassifier())],\n                   final_estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(cv=5,\n                   estimators=[(&#x27;qda&#x27;, QuadraticDiscriminantAnalysis()),\n                               (&#x27;svc&#x27;, SVC(probability=True)),\n                               (&#x27;lgbm&#x27;, LGBMClassifier())],\n                   final_estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>qda</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">QuadraticDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>QuadraticDiscriminantAnalysis()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgbm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# Make predictions on validation and test sets\nval_predictions = stacking_model.predict(X_val)\ntest_predictions = stacking_model.predict(X_test)\n\n# Probability predictions for ROC and AUPRC\nif hasattr(stacking_model, \"predict_proba\"):\n    val_prob = stacking_model.predict_proba(X_val)\n    test_prob = stacking_model.predict_proba(X_test)\nelse:\n    val_prob = None\n    test_prob = None\n\n# Calculate metrics for validation set\nval_metrics = calculate_metrics(y_val, val_predictions, val_prob)\ntest_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n\n# Print stacking model metrics\nprint(\"\\nStacking Model Validation Metrics:\")\nfor metric, value in val_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\nprint(\"\\nStacking Model Test Metrics:\")\nfor metric, value in test_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:12:58.757173Z","iopub.execute_input":"2024-09-25T19:12:58.757759Z","iopub.status.idle":"2024-09-25T19:13:06.485266Z","shell.execute_reply.started":"2024-09-25T19:12:58.757729Z","shell.execute_reply":"2024-09-25T19:13:06.484310Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\nStacking Model Validation Metrics:\naccuracy: 0.9735\nf1_score: 0.9624\nmcc: 0.9419\nsensitivity: 0.9624\nspecificity: 0.9796\nfdr: 0.0376\nauroc: 0.9959\nauprc: 0.9924\n\nStacking Model Test Metrics:\naccuracy: 0.9862\nf1_score: 0.9813\nmcc: 0.9704\nsensitivity: 0.9806\nspecificity: 0.9895\nfdr: 0.0179\nauroc: 0.9988\nauprc: 0.9976\n","output_type":"stream"}]},{"cell_type":"code","source":"# Perform cross-validation for stacking model using the training set\nn_folds = 5  # You can adjust this number\ncv_scores = cross_val_score(stacking_model, X_train, y_train, cv=n_folds, scoring='accuracy')\n\n# Calculate average cross-validation score\naverage_cv_score = cv_scores.mean()\nstd_cv_score = cv_scores.std()\n\n# Print cross-validation results\nprint(\"\\nCross-Validation Scores for Stacking Model:\")\nprint(f\"Scores: {cv_scores}\")\nprint(f\"Average Score: {average_cv_score:.4f} ± {std_cv_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:13:21.509442Z","iopub.execute_input":"2024-09-25T19:13:21.509806Z","iopub.status.idle":"2024-09-25T19:27:12.256604Z","shell.execute_reply.started":"2024-09-25T19:13:21.509776Z","shell.execute_reply":"2024-09-25T19:27:12.255562Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2187, number of negative: 3608\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116157 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377394 -> initscore=-0.500623\n[LightGBM] [Info] Start training from score -0.500623\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089489 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088793 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2886\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087847 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377481 -> initscore=-0.500256\n[LightGBM] [Info] Start training from score -0.500256\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2886\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377481 -> initscore=-0.500256\n[LightGBM] [Info] Start training from score -0.500256\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2886\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088809 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377481 -> initscore=-0.500256\n[LightGBM] [Info] Start training from score -0.500256\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088610 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089134 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088494 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088904 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1748, number of negative: 2888\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087723 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377049 -> initscore=-0.502092\n[LightGBM] [Info] Start training from score -0.502092\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118271 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087943 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088503 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088239 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087340 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1748, number of negative: 2888\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087948 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377049 -> initscore=-0.502092\n[LightGBM] [Info] Start training from score -0.502092\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116537 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089353 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088005 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088894 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088656 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1748, number of negative: 2888\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089130 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377049 -> initscore=-0.502092\n[LightGBM] [Info] Start training from score -0.502092\n[LightGBM] [Info] Number of positive: 2187, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140147 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5796, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377329 -> initscore=-0.500900\n[LightGBM] [Info] Start training from score -0.500900\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087017 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.105815 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4637, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377399 -> initscore=-0.500602\n[LightGBM] [Info] Start training from score -0.500602\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087646 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4637, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377399 -> initscore=-0.500602\n[LightGBM] [Info] Start training from score -0.500602\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087437 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4637, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377399 -> initscore=-0.500602\n[LightGBM] [Info] Start training from score -0.500602\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2888\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.087754 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4637, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377184 -> initscore=-0.501520\n[LightGBM] [Info] Start training from score -0.501520\n\nCross-Validation Scores for Stacking Model:\nScores: [0.97101449 0.97377502 0.97653554 0.96687371 0.97237569]\nAverage Score: 0.9721 ± 0.0032\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## proteins","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertForMaskedLM, BertTokenizer, BertModel\n\n# Load the CSV file\n# file_path = 'path/to/your/combined_peptides.csv'\ndata = pd.read_csv('/kaggle/input/new-combined-data/combined_protein.csv')\n\n# Load the pretrained ProtBERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert', do_lower_case=False)\nmodel = BertModel.from_pretrained('Rostlab/prot_bert')\nmodel.to('cuda')\n\ndef add_spaces(s):\n    return ' '.join(s)\n\n# Prepare the sequence data\nsequences = data['sequence'].tolist()\nsequences = [add_spaces(s) for s in sequences]\nlabels = data['label'].tolist()\n\n# Tokenize sequences\ndef tokenize_sequences(sequences, tokenizer, max_length=512):\n    tokenized_sequences = []\n    for seq in sequences:\n        # Tokenize and encode the sequence\n        inputs = tokenizer(seq, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n        tokenized_sequences.append(inputs)\n    return tokenized_sequences\n\ntokenized_sequences = tokenize_sequences(sequences, tokenizer)\n\n# Extract features\n# Extract features\ndef extract_features(tokenized_sequences, model):\n    model.eval()\n    all_embeddings = []\n    with torch.no_grad():\n        for tokenized_sequence in tokenized_sequences:\n            # Move tokenized sequence to CUDA\n            tokenized_sequence = {key: val.to('cuda') for key, val in tokenized_sequence.items()}\n            # Get model outputs\n            outputs = model(**tokenized_sequence)\n            # Compute mean pooling on the token embeddings\n            sequence_embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n            all_embeddings.append(sequence_embeddings)\n    return all_embeddings\n\nsequence_embeddings = extract_features(tokenized_sequences, model)\n\n# Convert the embeddings to a DataFrame\nembedding_df = pd.DataFrame(sequence_embeddings)\nembedding_df['label'] = labels\n\nembedding_df.to_csv('/kaggle/working/embeddings_pro.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:23:47.530774Z","iopub.execute_input":"2024-09-25T18:23:47.531578Z","iopub.status.idle":"2024-09-25T18:36:36.378436Z","shell.execute_reply.started":"2024-09-25T18:23:47.531533Z","shell.execute_reply":"2024-09-25T18:36:36.377309Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load the embeddings\nembedding_df = pd.read_csv('/kaggle/working/embeddings_pro.csv')\n\n# Split the data into features and labels\nX = embedding_df.drop(columns=['label'])\ny = embedding_df['label']\n\n# First split into training and temporary (test + validation) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Second split to divide temporary set into validation and test sets\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Save the split datasets\nX_train.to_csv('/kaggle/working/X_train_pro.csv', index=False)\nX_test.to_csv('/kaggle/working/X_test_pro.csv', index=False)\ny_train.to_csv('/kaggle/working/y_train_pro.csv', index=False)\ny_test.to_csv('/kaggle/working/y_test_pro.csv', index=False)\nX_val.to_csv('/kaggle/working/X_val_pro.csv', index=False)\ny_val.to_csv('/kaggle/working/y_val_pro.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:36:36.379969Z","iopub.execute_input":"2024-09-25T18:36:36.380634Z","iopub.status.idle":"2024-09-25T18:36:51.849760Z","shell.execute_reply.started":"2024-09-25T18:36:36.380600Z","shell.execute_reply":"2024-09-25T18:36:51.848953Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from lazypredict.Supervised import LazyClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the split datasets\nX_train = pd.read_csv('/kaggle/working/X_train_pro.csv')\nX_test = pd.read_csv('/kaggle/working/X_test_pro.csv')\nX_val = pd.read_csv('/kaggle/working/X_val_pro.csv')\ny_train = pd.read_csv('/kaggle/working/y_train_pro.csv').values.ravel()\ny_test = pd.read_csv('/kaggle/working/y_test_pro.csv').values.ravel()\ny_val = pd.read_csv('/kaggle/working/y_val_pro.csv').values.ravel()\n\n# Initialize LazyClassifier\nclf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n\n# Train and test the models\nmodels, predictions = clf.fit(X_train, X_val, y_train, y_val)\n\n# Display the results\nprint(models)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:36:51.850865Z","iopub.execute_input":"2024-09-25T18:36:51.851162Z","iopub.status.idle":"2024-09-25T18:40:12.823461Z","shell.execute_reply.started":"2024-09-25T18:36:51.851137Z","shell.execute_reply":"2024-09-25T18:40:12.822541Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":" 97%|█████████▋| 28/29 [03:07<00:04,  4.01s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2053, number of negative: 3380\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107432 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5433, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377876 -> initscore=-0.498574\n[LightGBM] [Info] Start training from score -0.498574\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 29/29 [03:18<00:00,  6.86s/it]","output_type":"stream"},{"name":"stdout","text":"                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\nModel                                                                           \nSVC                                0.97               0.97     0.97      0.97   \nXGBClassifier                      0.97               0.97     0.97      0.97   \nLGBMClassifier                     0.97               0.97     0.97      0.97   \nLinearDiscriminantAnalysis         0.97               0.97     0.97      0.97   \nRidgeClassifier                    0.97               0.97     0.97      0.97   \nRidgeClassifierCV                  0.97               0.97     0.97      0.97   \nKNeighborsClassifier               0.96               0.97     0.97      0.96   \nExtraTreesClassifier               0.97               0.96     0.96      0.97   \nRandomForestClassifier             0.97               0.96     0.96      0.97   \nPassiveAggressiveClassifier        0.96               0.96     0.96      0.96   \nLogisticRegression                 0.96               0.96     0.96      0.96   \nCalibratedClassifierCV             0.96               0.96     0.96      0.96   \nPerceptron                         0.96               0.96     0.96      0.96   \nLinearSVC                          0.95               0.96     0.96      0.95   \nSGDClassifier                      0.96               0.96     0.96      0.96   \nBaggingClassifier                  0.95               0.94     0.94      0.95   \nAdaBoostClassifier                 0.94               0.94     0.94      0.94   \nDecisionTreeClassifier             0.93               0.93     0.93      0.93   \nNuSVC                              0.93               0.91     0.91      0.93   \nExtraTreeClassifier                0.91               0.91     0.91      0.91   \nQuadraticDiscriminantAnalysis      0.92               0.88     0.88      0.91   \nGaussianNB                         0.88               0.88     0.88      0.88   \nBernoulliNB                        0.87               0.87     0.87      0.87   \nNearestCentroid                    0.86               0.86     0.86      0.86   \nLabelPropagation                   0.77               0.68     0.68      0.74   \nLabelSpreading                     0.77               0.68     0.68      0.74   \nDummyClassifier                    0.64               0.50     0.50      0.50   \n\n                               Time Taken  \nModel                                      \nSVC                                  4.19  \nXGBClassifier                        9.83  \nLGBMClassifier                      11.12  \nLinearDiscriminantAnalysis           1.45  \nRidgeClassifier                      0.41  \nRidgeClassifierCV                    1.30  \nKNeighborsClassifier                 0.54  \nExtraTreesClassifier                 1.95  \nRandomForestClassifier              15.48  \nPassiveAggressiveClassifier          0.87  \nLogisticRegression                   0.73  \nCalibratedClassifierCV               9.80  \nPerceptron                           0.57  \nLinearSVC                            2.50  \nSGDClassifier                        0.75  \nBaggingClassifier                   66.58  \nAdaBoostClassifier                  35.80  \nDecisionTreeClassifier              11.84  \nNuSVC                               16.13  \nExtraTreeClassifier                  0.24  \nQuadraticDiscriminantAnalysis        2.08  \nGaussianNB                           0.30  \nBernoulliNB                          0.44  \nNearestCentroid                      0.32  \nLabelPropagation                     1.49  \nLabelSpreading                       1.77  \nDummyClassifier                      0.21  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc, confusion_matrix, classification_report\n\n# Define the model dictionary including AdaBoostClassifier\nmodel_dict = {\n    'LGBMClassifier': LGBMClassifier(),\n    'XGBClassifier': XGBClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'LogisticRegression': LogisticRegression(),\n    'SVC': SVC(probability=True),  # SVC needs probability=True for AUROC\n    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n}\n\n# Function to calculate additional metrics\ndef calculate_metrics(y_true, y_pred, y_prob=None):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    mcc = matthews_corrcoef(y_true, y_pred)\n    \n    sensitivity = tp / (tp + fn)  # Sensitivity (SN)\n    specificity = tn / (tn + fp)  # Specificity (SP)\n    fdr = fp / (fp + tp)  # False Discovery Rate (FDR)\n    \n    if y_prob is not None:\n        auroc = roc_auc_score(y_true, y_prob[:, 1])\n        precision, recall, _ = precision_recall_curve(y_true, y_prob[:, 1])\n        auprc = auc(recall, precision)\n    else:\n        auroc = None\n        auprc = None\n\n    return {\n        'accuracy': accuracy,\n        'f1_score': f1,\n        'mcc': mcc,\n        'sensitivity': sensitivity,\n        'specificity': specificity,\n        'fdr': fdr,\n        'auroc': auroc,\n        'auprc': auprc\n    }\n\n# Fit and evaluate each model\nbest_model_name = None\nbest_accuracy = 0\nresults = {}\n\nfor model_name, model in model_dict.items():\n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    val_predictions = model.predict(X_val)\n    test_predictions = model.predict(X_test)\n    \n    # Probability predictions for ROC and AUPRC\n    if hasattr(model, \"predict_proba\"):\n        val_prob = model.predict_proba(X_val)\n        test_prob = model.predict_proba(X_test)\n    else:\n        val_prob = None\n        test_prob = None\n    \n    # Calculate metrics for validation set\n    val_metrics = calculate_metrics(y_val, val_predictions, val_prob)\n    test_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n    \n    # Store results\n    results[model_name] = {\n        'validation_metrics': val_metrics,\n        'test_metrics': test_metrics\n    }\n    \n    # Print metrics\n    print(f\"\\n{model_name} Validation Metrics:\")\n    for metric, value in val_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    print(f\"\\n{model_name} Test Metrics:\")\n    for metric, value in test_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    # Track the best model by accuracy\n    if val_metrics['accuracy'] > best_accuracy:\n        best_accuracy = val_metrics['accuracy']\n        best_model_name = model_name\n\nprint(f\"\\nBest Model Name: {best_model_name}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T18:40:12.826119Z","iopub.execute_input":"2024-09-25T18:40:12.826410Z","iopub.status.idle":"2024-09-25T18:41:54.802886Z","shell.execute_reply.started":"2024-09-25T18:40:12.826383Z","shell.execute_reply":"2024-09-25T18:41:54.801662Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2053, number of negative: 3380\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109615 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5433, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377876 -> initscore=-0.498574\n[LightGBM] [Info] Start training from score -0.498574\n\nLGBMClassifier Validation Metrics:\naccuracy: 0.9680\nf1_score: 0.9557\nmcc: 0.9308\nsensitivity: 0.9660\nspecificity: 0.9690\nfdr: 0.0544\nauroc: 0.9938\nauprc: 0.9862\n\nLGBMClassifier Test Metrics:\naccuracy: 0.9719\nf1_score: 0.9620\nmcc: 0.9397\nsensitivity: 0.9642\nspecificity: 0.9764\nfdr: 0.0401\nauroc: 0.9962\nauprc: 0.9930\n\nXGBClassifier Validation Metrics:\naccuracy: 0.9685\nf1_score: 0.9565\nmcc: 0.9320\nsensitivity: 0.9676\nspecificity: 0.9690\nfdr: 0.0543\nauroc: 0.9932\nauprc: 0.9856\n\nXGBClassifier Test Metrics:\naccuracy: 0.9730\nf1_score: 0.9634\nmcc: 0.9420\nsensitivity: 0.9627\nspecificity: 0.9790\nfdr: 0.0359\nauroc: 0.9962\nauprc: 0.9931\n\nRandomForestClassifier Validation Metrics:\naccuracy: 0.9647\nf1_score: 0.9508\nmcc: 0.9233\nsensitivity: 0.9552\nspecificity: 0.9699\nfdr: 0.0535\nauroc: 0.9922\nauprc: 0.9862\n\nRandomForestClassifier Test Metrics:\naccuracy: 0.9680\nf1_score: 0.9565\nmcc: 0.9312\nsensitivity: 0.9507\nspecificity: 0.9781\nfdr: 0.0378\nauroc: 0.9962\nauprc: 0.9933\n\nAdaBoostClassifier Validation Metrics:\naccuracy: 0.9459\nf1_score: 0.9254\nmcc: 0.8832\nsensitivity: 0.9383\nspecificity: 0.9501\nfdr: 0.0871\nauroc: 0.9851\nauprc: 0.9708\n\nAdaBoostClassifier Test Metrics:\naccuracy: 0.9432\nf1_score: 0.9225\nmcc: 0.8777\nsensitivity: 0.9149\nspecificity: 0.9597\nfdr: 0.0698\nauroc: 0.9868\nauprc: 0.9752\n\nLogisticRegression Validation Metrics:\naccuracy: 0.9481\nf1_score: 0.9288\nmcc: 0.8883\nsensitivity: 0.9460\nspecificity: 0.9493\nfdr: 0.0878\nauroc: 0.9872\nauprc: 0.9737\n\nLogisticRegression Test Metrics:\naccuracy: 0.9514\nf1_score: 0.9341\nmcc: 0.8957\nsensitivity: 0.9313\nspecificity: 0.9632\nfdr: 0.0631\nauroc: 0.9914\nauprc: 0.9851\n\nSVC Validation Metrics:\naccuracy: 0.9459\nf1_score: 0.9255\nmcc: 0.8833\nsensitivity: 0.9398\nspecificity: 0.9493\nfdr: 0.0883\nauroc: 0.9858\nauprc: 0.9721\n\nSVC Test Metrics:\naccuracy: 0.9437\nf1_score: 0.9232\nmcc: 0.8789\nsensitivity: 0.9149\nspecificity: 0.9606\nfdr: 0.0684\nauroc: 0.9885\nauprc: 0.9805\n\nQuadraticDiscriminantAnalysis Validation Metrics:\naccuracy: 0.9155\nf1_score: 0.8673\nmcc: 0.8202\nsensitivity: 0.7716\nspecificity: 0.9957\nfdr: 0.0099\nauroc: 0.9349\nauprc: 0.9542\n\nQuadraticDiscriminantAnalysis Test Metrics:\naccuracy: 0.9211\nf1_score: 0.8809\nmcc: 0.8355\nsensitivity: 0.7896\nspecificity: 0.9982\nfdr: 0.0038\nauroc: 0.9429\nauprc: 0.9623\n\nBest Model Name: XGBClassifier\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Define base models for stacking\nbase_models = [\n    ('lgbm', LGBMClassifier()),  # LightGBM model\n    ('xgb', XGBClassifier(tree_method='gpu_hist', gpu_id=0)),  # XGBoost model with GPU\n    ('svc', SVC(probability=True))  # Support Vector Classifier\n]\n\n# Create stacking classifier\nstacking_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression(),  # You can choose any classifier here\n    cv=5  # Use 5-fold cross-validation\n)\n\n# Fit the stacking model\nstacking_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:45:46.729490Z","iopub.execute_input":"2024-09-25T19:45:46.730367Z","iopub.status.idle":"2024-09-25T19:49:49.300083Z","shell.execute_reply.started":"2024-09-25T19:45:46.730334Z","shell.execute_reply":"2024-09-25T19:49:49.299206Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2733, number of negative: 4511\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121897 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 7244, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377278 -> initscore=-0.501119\n[LightGBM] [Info] Start training from score -0.501119\n[LightGBM] [Info] Number of positive: 2187, number of negative: 3608\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093791 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377394 -> initscore=-0.500623\n[LightGBM] [Info] Start training from score -0.500623\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095522 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091085 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090585 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 2187, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5796, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377329 -> initscore=-0.500900\n[LightGBM] [Info] Start training from score -0.500900\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"StackingClassifier(cv=5,\n                   estimators=[('lgbm', LGBMClassifier()),\n                               ('xgb',\n                                XGBClassifier(base_score=None, booster=None,\n                                              callbacks=None,\n                                              colsample_bylevel=None,\n                                              colsample_bynode=None,\n                                              colsample_bytree=None,\n                                              device=None,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None,\n                                              feature_types=None, gamma=None,\n                                              gpu_id=0, grow_policy=None,\n                                              importance_type=None,\n                                              interaction_constraints=None,\n                                              learning_rate=None, max_bin=None,\n                                              max_cat_threshold=None,\n                                              max_cat_to_onehot=None,\n                                              max_delta_step=None,\n                                              max_depth=None, max_leaves=None,\n                                              min_child_weight=None,\n                                              missing=nan,\n                                              monotone_constraints=None,\n                                              multi_strategy=None,\n                                              n_estimators=None, n_jobs=None,\n                                              num_parallel_tree=None, ...)),\n                               ('svc', SVC(probability=True))],\n                   final_estimator=LogisticRegression())","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(cv=5,\n                   estimators=[(&#x27;lgbm&#x27;, LGBMClassifier()),\n                               (&#x27;xgb&#x27;,\n                                XGBClassifier(base_score=None, booster=None,\n                                              callbacks=None,\n                                              colsample_bylevel=None,\n                                              colsample_bynode=None,\n                                              colsample_bytree=None,\n                                              device=None,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None,\n                                              feature_types=None, gamma=None,\n                                              gpu_id=0, grow_policy=None,\n                                              importance_type=None,\n                                              interaction_constraints=None,\n                                              learning_rate=None, max_bin=None,\n                                              max_cat_threshold=None,\n                                              max_cat_to_onehot=None,\n                                              max_delta_step=None,\n                                              max_depth=None, max_leaves=None,\n                                              min_child_weight=None,\n                                              missing=nan,\n                                              monotone_constraints=None,\n                                              multi_strategy=None,\n                                              n_estimators=None, n_jobs=None,\n                                              num_parallel_tree=None, ...)),\n                               (&#x27;svc&#x27;, SVC(probability=True))],\n                   final_estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(cv=5,\n                   estimators=[(&#x27;lgbm&#x27;, LGBMClassifier()),\n                               (&#x27;xgb&#x27;,\n                                XGBClassifier(base_score=None, booster=None,\n                                              callbacks=None,\n                                              colsample_bylevel=None,\n                                              colsample_bynode=None,\n                                              colsample_bytree=None,\n                                              device=None,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None,\n                                              feature_types=None, gamma=None,\n                                              gpu_id=0, grow_policy=None,\n                                              importance_type=None,\n                                              interaction_constraints=None,\n                                              learning_rate=None, max_bin=None,\n                                              max_cat_threshold=None,\n                                              max_cat_to_onehot=None,\n                                              max_delta_step=None,\n                                              max_depth=None, max_leaves=None,\n                                              min_child_weight=None,\n                                              missing=nan,\n                                              monotone_constraints=None,\n                                              multi_strategy=None,\n                                              n_estimators=None, n_jobs=None,\n                                              num_parallel_tree=None, ...)),\n                               (&#x27;svc&#x27;, SVC(probability=True))],\n                   final_estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgbm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=0, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, ...)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# Make predictions on validation and test sets\nval_predictions = stacking_model.predict(X_val)\ntest_predictions = stacking_model.predict(X_test)\n\n# Probability predictions for ROC and AUPRC\nif hasattr(stacking_model, \"predict_proba\"):\n    val_prob = stacking_model.predict_proba(X_val)\n    test_prob = stacking_model.predict_proba(X_test)\nelse:\n    val_prob = None\n    test_prob = None\n\n# Calculate metrics for validation set\nval_metrics = calculate_metrics(y_val, val_predictions, val_prob)\ntest_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n\n# Print stacking model metrics\nprint(\"\\nStacking Model Validation Metrics:\")\nfor metric, value in val_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\nprint(\"\\nStacking Model Test Metrics:\")\nfor metric, value in test_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:49:57.278421Z","iopub.execute_input":"2024-09-25T19:49:57.279035Z","iopub.status.idle":"2024-09-25T19:50:05.264087Z","shell.execute_reply.started":"2024-09-25T19:49:57.278991Z","shell.execute_reply":"2024-09-25T19:50:05.262948Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"\nStacking Model Validation Metrics:\naccuracy: 0.9752\nf1_score: 0.9648\nmcc: 0.9456\nsensitivity: 0.9655\nspecificity: 0.9804\nfdr: 0.0360\nauroc: 0.9948\nauprc: 0.9895\n\nStacking Model Test Metrics:\naccuracy: 0.9868\nf1_score: 0.9821\nmcc: 0.9716\nsensitivity: 0.9806\nspecificity: 0.9904\nfdr: 0.0165\nauroc: 0.9984\nauprc: 0.9967\n","output_type":"stream"}]},{"cell_type":"code","source":"# Perform cross-validation for stacking model using the training set\nn_folds = 5  # You can adjust this number\ncv_scores = cross_val_score(stacking_model, X_train, y_train, cv=n_folds, scoring='accuracy')\n\n# Calculate average cross-validation score\naverage_cv_score = cv_scores.mean()\nstd_cv_score = cv_scores.std()\n\n# Print cross-validation results\nprint(\"\\nCross-Validation Scores for Stacking Model:\")\nprint(f\"Scores: {cv_scores}\")\nprint(f\"Average Score: {average_cv_score:.4f} ± {std_cv_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T19:51:20.016550Z","iopub.execute_input":"2024-09-25T19:51:20.017246Z","iopub.status.idle":"2024-09-25T20:05:50.173891Z","shell.execute_reply.started":"2024-09-25T19:51:20.017215Z","shell.execute_reply":"2024-09-25T20:05:50.171900Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2187, number of negative: 3608\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093673 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377394 -> initscore=-0.500623\n[LightGBM] [Info] Start training from score -0.500623\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058930 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058775 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2886\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377481 -> initscore=-0.500256\n[LightGBM] [Info] Start training from score -0.500256\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2886\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377481 -> initscore=-0.500256\n[LightGBM] [Info] Start training from score -0.500256\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2886\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059802 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377481 -> initscore=-0.500256\n[LightGBM] [Info] Start training from score -0.500256\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095169 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060084 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059436 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060653 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059391 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1748, number of negative: 2888\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058411 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377049 -> initscore=-0.502092\n[LightGBM] [Info] Start training from score -0.502092\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058259 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058785 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058275 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059643 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1748, number of negative: 2888\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377049 -> initscore=-0.502092\n[LightGBM] [Info] Start training from score -0.502092\n[LightGBM] [Info] Number of positive: 2186, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5795, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377222 -> initscore=-0.501357\n[LightGBM] [Info] Start training from score -0.501357\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057976 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059856 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058201 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1748, number of negative: 2888\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377049 -> initscore=-0.502092\n[LightGBM] [Info] Start training from score -0.502092\n[LightGBM] [Info] Number of positive: 2187, number of negative: 3609\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094607 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 5796, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377329 -> initscore=-0.500900\n[LightGBM] [Info] Start training from score -0.500900\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061035 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4636, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377265 -> initscore=-0.501174\n[LightGBM] [Info] Start training from score -0.501174\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059301 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4637, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377399 -> initscore=-0.500602\n[LightGBM] [Info] Start training from score -0.500602\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059612 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4637, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377399 -> initscore=-0.500602\n[LightGBM] [Info] Start training from score -0.500602\n[LightGBM] [Info] Number of positive: 1750, number of negative: 2887\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4637, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377399 -> initscore=-0.500602\n[LightGBM] [Info] Start training from score -0.500602\n[LightGBM] [Info] Number of positive: 1749, number of negative: 2888\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058424 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 261120\n[LightGBM] [Info] Number of data points in the train set: 4637, number of used features: 1024\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.377184 -> initscore=-0.501520\n[LightGBM] [Info] Start training from score -0.501520\n\nCross-Validation Scores for Stacking Model:\nScores: [0.97101449 0.97515528 0.97515528 0.96963423 0.97168508]\nAverage Score: 0.9725 ± 0.0022\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}