{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8812025,"sourceType":"datasetVersion","datasetId":5300559},{"sourceId":8916544,"sourceType":"datasetVersion","datasetId":5362312},{"sourceId":9098077,"sourceType":"datasetVersion","datasetId":5490741},{"sourceId":9481521,"sourceType":"datasetVersion","datasetId":5767385}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install fair-esm pandas scikit-learn torch lazypredict\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:13:27.397605Z","iopub.execute_input":"2024-09-25T20:13:27.398501Z","iopub.status.idle":"2024-09-25T20:13:41.128577Z","shell.execute_reply.started":"2024-09-25T20:13:27.398465Z","shell.execute_reply":"2024-09-25T20:13:41.127351Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting fair-esm\n  Downloading fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nCollecting lazypredict\n  Downloading lazypredict-0.2.12-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.5.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from lazypredict) (8.1.7)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from lazypredict) (4.66.4)\nRequirement already satisfied: lightgbm in /opt/conda/lib/python3.10/site-packages (from lazypredict) (4.2.0)\nRequirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (from lazypredict) (2.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\nInstalling collected packages: fair-esm, lazypredict\nSuccessfully installed fair-esm-2.0.0 lazypredict-0.2.12\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport esm\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:13:45.111526Z","iopub.execute_input":"2024-09-25T20:13:45.111934Z","iopub.status.idle":"2024-09-25T20:13:48.750923Z","shell.execute_reply.started":"2024-09-25T20:13:45.111901Z","shell.execute_reply":"2024-09-25T20:13:48.749980Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"ESM Feature Extraction**","metadata":{}},{"cell_type":"code","source":"\n\n# Load the CSV file\nfile_path = '/kaggle/input/combined-dataset/combined_peptides.csv'\ndata = pd.read_csv(file_path)\n\n# Load the pretrained ESM model\nmodel, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()  # Adjust model as needed\nbatch_converter = alphabet.get_batch_converter()\n\n# Move model to GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Prepare the sequence data\nsequences = data['sequence'].tolist()\nlabels = data['label'].tolist()\n\nbatch_size = 100  # Adjust the batch size based on your memory capacity\nall_embeddings = []\n\nfor i in range(0, len(sequences), batch_size):\n    batch_sequences = sequences[i:i+batch_size]\n    batch_labels = labels[i:i+batch_size]\n    \n    data_tuples = [(f\"sequence_{j}\", seq) for j, seq in enumerate(batch_sequences)]\n    _, _, batch_tokens = batch_converter(data_tuples)\n    \n    # Move batch tokens to GPU\n    batch_tokens = batch_tokens.to(device)\n    \n    with torch.no_grad():\n        results = model(batch_tokens, repr_layers=[33])\n    token_embeddings = results[\"representations\"][33]\n    \n    # Move embeddings to CPU for further processing\n    token_embeddings = token_embeddings.cpu()\n\n    # Mean pooling to get fixed-size feature vectors\n    def mean_pooling(token_embeddings, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings / sum_mask\n\n    attention_mask = batch_tokens.ne(alphabet.padding_idx)\n    attention_mask = attention_mask.cpu()  # Ensure attention mask is also on CPU\n    sequence_embeddings = mean_pooling(token_embeddings, attention_mask)\n    \n    # Move sequence embeddings to CPU before converting to numpy\n    all_embeddings.append(sequence_embeddings.cpu().numpy())\n\n# Concatenate all the embeddings\nall_embeddings = np.concatenate(all_embeddings, axis=0)\n\n# Convert the embeddings to a DataFrame\nembedding_df = pd.DataFrame(all_embeddings)\nembedding_df['label'] = labels\n\n# Save the DataFrame to a CSV file\nembedding_df.to_csv('/kaggle/working/embeddings.csv', index=False)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T18:53:39.432318Z","iopub.execute_input":"2024-09-08T18:53:39.432786Z","iopub.status.idle":"2024-09-08T18:55:42.799893Z","shell.execute_reply.started":"2024-09-08T18:53:39.432748Z","shell.execute_reply":"2024-09-08T18:55:42.799101Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm1b_t33_650M_UR50S.pt\" to /root/.cache/torch/hub/checkpoints/esm1b_t33_650M_UR50S.pt\nDownloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm1b_t33_650M_UR50S-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm1b_t33_650M_UR50S-contact-regression.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load the embeddings\nembedding_df = pd.read_csv('/kaggle/input/embeddings/embeddings_esm.csv')\n\n# Split the data into features and labels\nX = embedding_df.drop(columns=['label'])\ny = embedding_df['label']\n\n# Split the data into training (60%) and testing (40%) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Save the split datasets\nX_train.to_csv('/kaggle/working/X_train.csv', index=False)\nX_test.to_csv('/kaggle/working/X_test.csv', index=False)\ny_train.to_csv('/kaggle/working/y_train.csv', index=False)\ny_test.to_csv('/kaggle/working/y_test.csv', index=False)\nX_val.to_csv('/kaggle/working/X_val.csv', index=False)\ny_val.to_csv('/kaggle/working/y_val.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:14:02.539289Z","iopub.execute_input":"2024-09-25T20:14:02.539800Z","iopub.status.idle":"2024-09-25T20:14:15.088773Z","shell.execute_reply.started":"2024-09-25T20:14:02.539769Z","shell.execute_reply":"2024-09-25T20:14:15.088018Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from lazypredict.Supervised import LazyClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the split datasets\nX_train = pd.read_csv('/kaggle/working/X_train.csv')\nX_test = pd.read_csv('/kaggle/working/X_test.csv')\nX_val = pd.read_csv('/kaggle/working/X_val.csv')\ny_train = pd.read_csv('/kaggle/working/y_train.csv').values.ravel()\ny_test = pd.read_csv('/kaggle/working/y_test.csv').values.ravel()\ny_val = pd.read_csv('/kaggle/working/y_val.csv').values.ravel()\n\n# Initialize LazyClassifier\nclf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n\n# Train and test the models\nmodels, predictions = clf.fit(X_train, X_val, y_train, y_val)\n\n# Display the results\nprint(models)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:14:21.709692Z","iopub.execute_input":"2024-09-25T20:14:21.710389Z","iopub.status.idle":"2024-09-25T20:17:09.726204Z","shell.execute_reply.started":"2024-09-25T20:14:21.710356Z","shell.execute_reply":"2024-09-25T20:17:09.725252Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":" 97%|█████████▋| 28/29 [02:31<00:04,  4.43s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 1812, number of negative: 1560\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043809 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 3372, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537367 -> initscore=0.149745\n[LightGBM] [Info] Start training from score 0.149745\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 29/29 [02:42<00:00,  5.61s/it]","output_type":"stream"},{"name":"stdout","text":"                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\nModel                                                                           \nXGBClassifier                      0.93               0.93     0.93      0.93   \nLGBMClassifier                     0.93               0.93     0.93      0.93   \nExtraTreesClassifier               0.93               0.93     0.93      0.93   \nSVC                                0.93               0.93     0.93      0.93   \nRandomForestClassifier             0.93               0.93     0.93      0.93   \nRidgeClassifierCV                  0.92               0.92     0.92      0.92   \nLinearDiscriminantAnalysis         0.91               0.91     0.91      0.91   \nRidgeClassifier                    0.91               0.91     0.91      0.91   \nCalibratedClassifierCV             0.91               0.91     0.91      0.91   \nPassiveAggressiveClassifier        0.91               0.91     0.91      0.91   \nBaggingClassifier                  0.91               0.91     0.91      0.91   \nKNeighborsClassifier               0.91               0.91     0.91      0.91   \nLogisticRegression                 0.91               0.91     0.91      0.91   \nLinearSVC                          0.91               0.91     0.91      0.91   \nSGDClassifier                      0.91               0.91     0.91      0.91   \nPerceptron                         0.90               0.90     0.90      0.90   \nQuadraticDiscriminantAnalysis      0.90               0.89     0.89      0.90   \nDecisionTreeClassifier             0.86               0.86     0.86      0.86   \nAdaBoostClassifier                 0.86               0.86     0.86      0.86   \nNuSVC                              0.85               0.85     0.85      0.85   \nExtraTreeClassifier                0.85               0.85     0.85      0.85   \nNearestCentroid                    0.82               0.83     0.83      0.82   \nGaussianNB                         0.82               0.82     0.82      0.82   \nBernoulliNB                        0.81               0.82     0.82      0.81   \nLabelPropagation                   0.64               0.67     0.67      0.60   \nLabelSpreading                     0.64               0.67     0.67      0.60   \nDummyClassifier                    0.54               0.50     0.50      0.38   \n\n                               Time Taken  \nModel                                      \nXGBClassifier                       12.43  \nLGBMClassifier                      11.30  \nExtraTreesClassifier                 1.76  \nSVC                                  4.31  \nRandomForestClassifier              10.17  \nRidgeClassifierCV                    1.87  \nLinearDiscriminantAnalysis           1.88  \nRidgeClassifier                      0.54  \nCalibratedClassifierCV              16.96  \nPassiveAggressiveClassifier          1.31  \nBaggingClassifier                   45.11  \nKNeighborsClassifier                 0.54  \nLogisticRegression                   0.70  \nLinearSVC                            4.30  \nSGDClassifier                        0.81  \nPerceptron                           0.63  \nQuadraticDiscriminantAnalysis        2.88  \nDecisionTreeClassifier               7.87  \nAdaBoostClassifier                  26.51  \nNuSVC                                7.08  \nExtraTreeClassifier                  0.24  \nNearestCentroid                      0.31  \nGaussianNB                           0.29  \nBernoulliNB                          0.42  \nLabelPropagation                     0.86  \nLabelSpreading                       1.02  \nDummyClassifier                      0.22  \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc, confusion_matrix, classification_report\n\n# Define the model dictionary including AdaBoostClassifier\nmodel_dict = {\n    'LGBMClassifier': LGBMClassifier(),\n    'XGBClassifier': XGBClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'LogisticRegression': LogisticRegression(),\n    'SVC': SVC(probability=True),  # SVC needs probability=True for AUROC\n    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n}\n\n# Function to calculate additional metrics\ndef calculate_metrics(y_true, y_pred, y_prob=None):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    mcc = matthews_corrcoef(y_true, y_pred)\n    \n    sensitivity = tp / (tp + fn)  # Sensitivity (SN)\n    specificity = tn / (tn + fp)  # Specificity (SP)\n    fdr = fp / (fp + tp)  # False Discovery Rate (FDR)\n    \n    if y_prob is not None:\n        auroc = roc_auc_score(y_true, y_prob[:, 1])\n        precision, recall, _ = precision_recall_curve(y_true, y_prob[:, 1])\n        auprc = auc(recall, precision)\n    else:\n        auroc = None\n        auprc = None\n\n    return {\n        'accuracy': accuracy,\n        'f1_score': f1,\n        'mcc': mcc,\n        'sensitivity': sensitivity,\n        'specificity': specificity,\n        'fdr': fdr,\n        'auroc': auroc,\n        'auprc': auprc\n    }\n\n# Fit and evaluate each model\nbest_model_name = None\nbest_accuracy = 0\nresults = {}\n\nfor model_name, model in model_dict.items():\n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    val_predictions = model.predict(X_val)\n    test_predictions = model.predict(X_test)\n    \n    # Probability predictions for ROC and AUPRC\n    if hasattr(model, \"predict_proba\"):\n        val_prob = model.predict_proba(X_val)\n        test_prob = model.predict_proba(X_test)\n    else:\n        val_prob = None\n        test_prob = None\n    \n    # Calculate metrics for validation set\n    val_metrics = calculate_metrics(y_val, val_predictions, val_prob)\n    test_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n    \n    # Store results\n    results[model_name] = {\n        'validation_metrics': val_metrics,\n        'test_metrics': test_metrics\n    }\n    \n    # Print metrics\n    print(f\"\\n{model_name} Validation Metrics:\")\n    for metric, value in val_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    print(f\"\\n{model_name} Test Metrics:\")\n    for metric, value in test_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    # Track the best model by accuracy\n    if val_metrics['accuracy'] > best_accuracy:\n        best_accuracy = val_metrics['accuracy']\n        best_model_name = model_name\n\nprint(f\"\\nBest Model Name: {best_model_name}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:18:02.347061Z","iopub.execute_input":"2024-09-25T20:18:02.347430Z","iopub.status.idle":"2024-09-25T20:19:29.043319Z","shell.execute_reply.started":"2024-09-25T20:18:02.347400Z","shell.execute_reply":"2024-09-25T20:19:29.042109Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 1812, number of negative: 1560\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042624 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 3372, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537367 -> initscore=0.149745\n[LightGBM] [Info] Start training from score 0.149745\n\nLGBMClassifier Validation Metrics:\naccuracy: 0.9297\nf1_score: 0.9341\nmcc: 0.8589\nsensitivity: 0.9272\nspecificity: 0.9327\nfdr: 0.0588\nauroc: 0.9802\nauprc: 0.9838\n\nLGBMClassifier Test Metrics:\naccuracy: 0.9289\nf1_score: 0.9350\nmcc: 0.8566\nsensitivity: 0.9319\nspecificity: 0.9252\nfdr: 0.0620\nauroc: 0.9734\nauprc: 0.9780\n\nXGBClassifier Validation Metrics:\naccuracy: 0.9306\nf1_score: 0.9351\nmcc: 0.8606\nsensitivity: 0.9305\nspecificity: 0.9308\nfdr: 0.0602\nauroc: 0.9797\nauprc: 0.9840\n\nXGBClassifier Test Metrics:\naccuracy: 0.9253\nf1_score: 0.9318\nmcc: 0.8493\nsensitivity: 0.9303\nspecificity: 0.9193\nfdr: 0.0667\nauroc: 0.9729\nauprc: 0.9772\n\nRandomForestClassifier Validation Metrics:\naccuracy: 0.9235\nf1_score: 0.9269\nmcc: 0.8482\nsensitivity: 0.9023\nspecificity: 0.9481\nfdr: 0.0472\nauroc: 0.9756\nauprc: 0.9823\n\nRandomForestClassifier Test Metrics:\naccuracy: 0.9200\nf1_score: 0.9256\nmcc: 0.8399\nsensitivity: 0.9076\nspecificity: 0.9350\nfdr: 0.0556\nauroc: 0.9715\nauprc: 0.9779\n\nAdaBoostClassifier Validation Metrics:\naccuracy: 0.8594\nf1_score: 0.8692\nmcc: 0.7173\nsensitivity: 0.8692\nspecificity: 0.8481\nfdr: 0.1308\nauroc: 0.9422\nauprc: 0.9523\n\nAdaBoostClassifier Test Metrics:\naccuracy: 0.8631\nf1_score: 0.8750\nmcc: 0.7237\nsensitivity: 0.8736\nspecificity: 0.8504\nfdr: 0.1236\nauroc: 0.9438\nauprc: 0.9515\n\nLogisticRegression Validation Metrics:\naccuracy: 0.9110\nf1_score: 0.9171\nmcc: 0.8211\nsensitivity: 0.9156\nspecificity: 0.9058\nfdr: 0.0814\nauroc: 0.9653\nauprc: 0.9714\n\nLogisticRegression Test Metrics:\naccuracy: 0.9102\nf1_score: 0.9174\nmcc: 0.8192\nsensitivity: 0.9092\nspecificity: 0.9114\nfdr: 0.0743\nauroc: 0.9614\nauprc: 0.9641\n\nSVC Validation Metrics:\naccuracy: 0.8496\nf1_score: 0.8487\nmcc: 0.7105\nsensitivity: 0.7848\nspecificity: 0.9250\nfdr: 0.0760\nauroc: 0.9227\nauprc: 0.9405\n\nSVC Test Metrics:\naccuracy: 0.8524\nf1_score: 0.8566\nmcc: 0.7125\nsensitivity: 0.8039\nspecificity: 0.9114\nfdr: 0.0832\nauroc: 0.9195\nauprc: 0.9319\n\nQuadraticDiscriminantAnalysis Validation Metrics:\naccuracy: 0.8968\nf1_score: 0.9099\nmcc: 0.7986\nsensitivity: 0.9702\nspecificity: 0.8115\nfdr: 0.1433\nauroc: 0.8966\nauprc: 0.9245\n\nQuadraticDiscriminantAnalysis Test Metrics:\naccuracy: 0.8978\nf1_score: 0.9121\nmcc: 0.7983\nsensitivity: 0.9676\nspecificity: 0.8130\nfdr: 0.1373\nauroc: 0.8959\nauprc: 0.9268\n\nBest Model Name: XGBClassifier\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Define base models for stacking\nbase_models = [\n    ('lgbm', LGBMClassifier()),  # LightGBM model\n    ('extra_trees', ExtraTreesClassifier()),  # Extra Trees Classifier\n    ('xgb', XGBClassifier(tree_method='gpu_hist', gpu_id=0))  # XGBoost model with GPU\n]\n\n# Create stacking classifier\nstacking_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression(),  # You can choose any classifier here\n    cv=5  # Use 5-fold cross-validation\n)\n\n# Fit the stacking model\nstacking_model.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:21:08.904988Z","iopub.execute_input":"2024-09-25T20:21:08.905358Z","iopub.status.idle":"2024-09-25T20:22:31.031409Z","shell.execute_reply.started":"2024-09-25T20:21:08.905329Z","shell.execute_reply":"2024-09-25T20:22:31.030134Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 1812, number of negative: 1560\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045064 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 3372, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537367 -> initscore=0.149745\n[LightGBM] [Info] Start training from score 0.149745\n[LightGBM] [Info] Number of positive: 1449, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034451 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2697, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537264 -> initscore=0.149331\n[LightGBM] [Info] Start training from score 0.149331\n[LightGBM] [Info] Number of positive: 1449, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043982 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2697, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537264 -> initscore=0.149331\n[LightGBM] [Info] Start training from score 0.149331\n[LightGBM] [Info] Number of positive: 1450, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035508 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2698, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537435 -> initscore=0.150021\n[LightGBM] [Info] Start training from score 0.150021\n[LightGBM] [Info] Number of positive: 1450, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034626 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2698, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537435 -> initscore=0.150021\n[LightGBM] [Info] Start training from score 0.150021\n[LightGBM] [Info] Number of positive: 1450, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035459 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2698, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537435 -> initscore=0.150021\n[LightGBM] [Info] Start training from score 0.150021\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"StackingClassifier(cv=5,\n                   estimators=[('lgbm', LGBMClassifier()),\n                               ('extra_trees', ExtraTreesClassifier()),\n                               ('xgb',\n                                XGBClassifier(base_score=None, booster=None,\n                                              callbacks=None,\n                                              colsample_bylevel=None,\n                                              colsample_bynode=None,\n                                              colsample_bytree=None,\n                                              device=None,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None,\n                                              feature_types=None, gamma=None,\n                                              gpu_id...\n                                              importance_type=None,\n                                              interaction_constraints=None,\n                                              learning_rate=None, max_bin=None,\n                                              max_cat_threshold=None,\n                                              max_cat_to_onehot=None,\n                                              max_delta_step=None,\n                                              max_depth=None, max_leaves=None,\n                                              min_child_weight=None,\n                                              missing=nan,\n                                              monotone_constraints=None,\n                                              multi_strategy=None,\n                                              n_estimators=None, n_jobs=None,\n                                              num_parallel_tree=None, ...))],\n                   final_estimator=LogisticRegression())","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(cv=5,\n                   estimators=[(&#x27;lgbm&#x27;, LGBMClassifier()),\n                               (&#x27;extra_trees&#x27;, ExtraTreesClassifier()),\n                               (&#x27;xgb&#x27;,\n                                XGBClassifier(base_score=None, booster=None,\n                                              callbacks=None,\n                                              colsample_bylevel=None,\n                                              colsample_bynode=None,\n                                              colsample_bytree=None,\n                                              device=None,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None,\n                                              feature_types=None, gamma=None,\n                                              gpu_id...\n                                              importance_type=None,\n                                              interaction_constraints=None,\n                                              learning_rate=None, max_bin=None,\n                                              max_cat_threshold=None,\n                                              max_cat_to_onehot=None,\n                                              max_delta_step=None,\n                                              max_depth=None, max_leaves=None,\n                                              min_child_weight=None,\n                                              missing=nan,\n                                              monotone_constraints=None,\n                                              multi_strategy=None,\n                                              n_estimators=None, n_jobs=None,\n                                              num_parallel_tree=None, ...))],\n                   final_estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(cv=5,\n                   estimators=[(&#x27;lgbm&#x27;, LGBMClassifier()),\n                               (&#x27;extra_trees&#x27;, ExtraTreesClassifier()),\n                               (&#x27;xgb&#x27;,\n                                XGBClassifier(base_score=None, booster=None,\n                                              callbacks=None,\n                                              colsample_bylevel=None,\n                                              colsample_bynode=None,\n                                              colsample_bytree=None,\n                                              device=None,\n                                              early_stopping_rounds=None,\n                                              enable_categorical=False,\n                                              eval_metric=None,\n                                              feature_types=None, gamma=None,\n                                              gpu_id...\n                                              importance_type=None,\n                                              interaction_constraints=None,\n                                              learning_rate=None, max_bin=None,\n                                              max_cat_threshold=None,\n                                              max_cat_to_onehot=None,\n                                              max_delta_step=None,\n                                              max_depth=None, max_leaves=None,\n                                              min_child_weight=None,\n                                              missing=nan,\n                                              monotone_constraints=None,\n                                              multi_strategy=None,\n                                              n_estimators=None, n_jobs=None,\n                                              num_parallel_tree=None, ...))],\n                   final_estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lgbm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>extra_trees</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesClassifier</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=0, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, ...)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# Make predictions on validation and test sets\nval_predictions = stacking_model.predict(X_val)\ntest_predictions = stacking_model.predict(X_test)\n\n# Probability predictions for ROC and AUPRC\nif hasattr(stacking_model, \"predict_proba\"):\n    val_prob = stacking_model.predict_proba(X_val)\n    test_prob = stacking_model.predict_proba(X_test)\nelse:\n    val_prob = None\n    test_prob = None\n\n# Calculate metrics for validation set\nval_metrics = calculate_metrics(y_val, val_predictions, val_prob)\ntest_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n\n# Print stacking model metrics\nprint(\"\\nStacking Model Validation Metrics:\")\nfor metric, value in val_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\nprint(\"\\nStacking Model Test Metrics:\")\nfor metric, value in test_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:23:03.157378Z","iopub.execute_input":"2024-09-25T20:23:03.158184Z","iopub.status.idle":"2024-09-25T20:23:04.240540Z","shell.execute_reply.started":"2024-09-25T20:23:03.158134Z","shell.execute_reply":"2024-09-25T20:23:04.239466Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\nStacking Model Validation Metrics:\naccuracy: 0.9270\nf1_score: 0.9310\nmcc: 0.8542\nsensitivity: 0.9156\nspecificity: 0.9404\nfdr: 0.0531\nauroc: 0.9782\nauprc: 0.9831\n\nStacking Model Test Metrics:\naccuracy: 0.9289\nf1_score: 0.9342\nmcc: 0.8573\nsensitivity: 0.9206\nspecificity: 0.9390\nfdr: 0.0518\nauroc: 0.9745\nauprc: 0.9800\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:23:45.537603Z","iopub.execute_input":"2024-09-25T20:23:45.537978Z","iopub.status.idle":"2024-09-25T20:23:45.542493Z","shell.execute_reply.started":"2024-09-25T20:23:45.537949Z","shell.execute_reply":"2024-09-25T20:23:45.541636Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Perform cross-validation for stacking model using the training set\nn_folds = 5  # You can adjust this number\ncv_scores = cross_val_score(stacking_model, X_train, y_train, cv=n_folds, scoring='accuracy')\n\n# Calculate average cross-validation score\naverage_cv_score = cv_scores.mean()\nstd_cv_score = cv_scores.std()\n\n# Print cross-validation results\nprint(\"\\nCross-Validation Scores for Stacking Model:\")\nprint(f\"Scores: {cv_scores}\")\nprint(f\"Average Score: {average_cv_score:.4f} ± {std_cv_score:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T20:23:46.686489Z","iopub.execute_input":"2024-09-25T20:23:46.686855Z","iopub.status.idle":"2024-09-25T20:29:55.000827Z","shell.execute_reply.started":"2024-09-25T20:23:46.686815Z","shell.execute_reply":"2024-09-25T20:29:54.999877Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 1449, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036126 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2697, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537264 -> initscore=0.149331\n[LightGBM] [Info] Start training from score 0.149331\n[LightGBM] [Info] Number of positive: 1159, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031044 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2157, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537320 -> initscore=0.149560\n[LightGBM] [Info] Start training from score 0.149560\n[LightGBM] [Info] Number of positive: 1159, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029827 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2157, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537320 -> initscore=0.149560\n[LightGBM] [Info] Start training from score 0.149560\n[LightGBM] [Info] Number of positive: 1159, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028538 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537071 -> initscore=0.148558\n[LightGBM] [Info] Start training from score 0.148558\n[LightGBM] [Info] Number of positive: 1159, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029251 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537071 -> initscore=0.148558\n[LightGBM] [Info] Start training from score 0.148558\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029076 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1449, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035817 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2697, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537264 -> initscore=0.149331\n[LightGBM] [Info] Start training from score 0.149331\n[LightGBM] [Info] Number of positive: 1159, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029052 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2157, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537320 -> initscore=0.149560\n[LightGBM] [Info] Start training from score 0.149560\n[LightGBM] [Info] Number of positive: 1159, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029155 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2157, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537320 -> initscore=0.149560\n[LightGBM] [Info] Start training from score 0.149560\n[LightGBM] [Info] Number of positive: 1159, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029157 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537071 -> initscore=0.148558\n[LightGBM] [Info] Start training from score 0.148558\n[LightGBM] [Info] Number of positive: 1159, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028957 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537071 -> initscore=0.148558\n[LightGBM] [Info] Start training from score 0.148558\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028912 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1450, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036097 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2698, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537435 -> initscore=0.150021\n[LightGBM] [Info] Start training from score 0.150021\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028715 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039075 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1160, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028790 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2159, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537286 -> initscore=0.149421\n[LightGBM] [Info] Start training from score 0.149421\n[LightGBM] [Info] Number of positive: 1160, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033599 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2159, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537286 -> initscore=0.149421\n[LightGBM] [Info] Start training from score 0.149421\n[LightGBM] [Info] Number of positive: 1450, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2698, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537435 -> initscore=0.150021\n[LightGBM] [Info] Start training from score 0.150021\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029041 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028854 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028624 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1160, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028579 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2159, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537286 -> initscore=0.149421\n[LightGBM] [Info] Start training from score 0.149421\n[LightGBM] [Info] Number of positive: 1160, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2159, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537286 -> initscore=0.149421\n[LightGBM] [Info] Start training from score 0.149421\n[LightGBM] [Info] Number of positive: 1450, number of negative: 1248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038947 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2698, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537435 -> initscore=0.150021\n[LightGBM] [Info] Start training from score 0.150021\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029382 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030481 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1160, number of negative: 998\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029730 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2158, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537535 -> initscore=0.150422\n[LightGBM] [Info] Start training from score 0.150422\n[LightGBM] [Info] Number of positive: 1160, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029343 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2159, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537286 -> initscore=0.149421\n[LightGBM] [Info] Start training from score 0.149421\n[LightGBM] [Info] Number of positive: 1160, number of negative: 999\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037820 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 326400\n[LightGBM] [Info] Number of data points in the train set: 2159, number of used features: 1280\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.537286 -> initscore=0.149421\n[LightGBM] [Info] Start training from score 0.149421\n\nCross-Validation Scores for Stacking Model:\nScores: [0.91851852 0.91555556 0.93768546 0.92136499 0.91839763]\nAverage Score: 0.9223 ± 0.0079\n","output_type":"stream"}]}]}