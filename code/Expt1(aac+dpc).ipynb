{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8916680,"sourceType":"datasetVersion","datasetId":5362429},{"sourceId":9088210,"sourceType":"datasetVersion","datasetId":5483863}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-27T09:11:04.365871Z","iopub.execute_input":"2024-09-27T09:11:04.366239Z","iopub.status.idle":"2024-09-27T09:11:05.461622Z","shell.execute_reply.started":"2024-09-27T09:11:04.366202Z","shell.execute_reply":"2024-09-27T09:11:05.460683Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/toxin-database/peptide.csv\n/kaggle/input/toxin-database/protein_test1002.csv\n/kaggle/input/toxin-database/protein_train1002.csv\n/kaggle/input/cobmineddb/combined_protein.csv\n/kaggle/input/cobmineddb/combined_peptides.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n# Load the dataset\nfile_path = '/kaggle/input/cobmineddb/combined_protein.csv'\nprotein_data = pd.read_csv(file_path)\nprotein_data","metadata":{"execution":{"iopub.status.busy":"2024-09-27T09:11:07.468498Z","iopub.execute_input":"2024-09-27T09:11:07.468976Z","iopub.status.idle":"2024-09-27T09:11:07.591451Z","shell.execute_reply.started":"2024-09-27T09:11:07.468941Z","shell.execute_reply":"2024-09-27T09:11:07.590534Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"              name                                           sequence  label  \\\n0     >HPCL1_XENTR  MGKQNSKLRPEVLQDLRENTEFTDHELQEWYKGFLKDCPTGHLTVE...      0   \n1      >SIX4_HOTTS  DGYIKGNKGCKVSCVINNVFCNSMCKSSGGSYGYCWSWGLACWCEG...      1   \n2     >TAG2L_AGEOR  MRAIISLLLISAMVFSIIEAVPEEEGLQLSEDERGGCLPHNRFCNA...      1   \n3     >P2011_DANRE  MKTKFTKKTVLKFFGILFAILLLSVLILFSVVIGRTFTFKVNRELG...      0   \n4      >O165_CONTE  MKLTCMVIVAVLFLTAWTFVTAITSNGLENLFPNAHHEMKNPEASK...      1   \n...            ...                                                ...    ...   \n9051  >EMC2B_XENLA  MSKVSDLYDVTWEDMRDKMKTWREDNYRNSEQIVDVGEELINEHAS...      0   \n9052   >COLI_LITCT  MLQPVWHACILAILGVFIFHVGEVRSQCWESNKCTDLSSEDGILEC...      0   \n9053  >MED30_DANRE  MTTPPLAQFSGQQQQQTQAARDVNTASLCRIGQETVQDIVLRTMEI...      0   \n9054  >RB87F_DROME  MAEQNDSNGNYDDGEEITEPEQLRKLFIGGLDYRTTDDGLKAHFEK...      0   \n9055   >RS17_SPOFR  MGRVRTKTVKKAAKIIIEKYYTRLTLDFDTNKRICEEIAIIPTKPL...      0   \n\n      sequence_length  \n0                 193  \n1                  62  \n2                  70  \n3                 515  \n4                  76  \n...               ...  \n9051              297  \n9052              263  \n9053              174  \n9054              385  \n9055              133  \n\n[9056 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>sequence</th>\n      <th>label</th>\n      <th>sequence_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&gt;HPCL1_XENTR</td>\n      <td>MGKQNSKLRPEVLQDLRENTEFTDHELQEWYKGFLKDCPTGHLTVE...</td>\n      <td>0</td>\n      <td>193</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&gt;SIX4_HOTTS</td>\n      <td>DGYIKGNKGCKVSCVINNVFCNSMCKSSGGSYGYCWSWGLACWCEG...</td>\n      <td>1</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&gt;TAG2L_AGEOR</td>\n      <td>MRAIISLLLISAMVFSIIEAVPEEEGLQLSEDERGGCLPHNRFCNA...</td>\n      <td>1</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&gt;P2011_DANRE</td>\n      <td>MKTKFTKKTVLKFFGILFAILLLSVLILFSVVIGRTFTFKVNRELG...</td>\n      <td>0</td>\n      <td>515</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&gt;O165_CONTE</td>\n      <td>MKLTCMVIVAVLFLTAWTFVTAITSNGLENLFPNAHHEMKNPEASK...</td>\n      <td>1</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9051</th>\n      <td>&gt;EMC2B_XENLA</td>\n      <td>MSKVSDLYDVTWEDMRDKMKTWREDNYRNSEQIVDVGEELINEHAS...</td>\n      <td>0</td>\n      <td>297</td>\n    </tr>\n    <tr>\n      <th>9052</th>\n      <td>&gt;COLI_LITCT</td>\n      <td>MLQPVWHACILAILGVFIFHVGEVRSQCWESNKCTDLSSEDGILEC...</td>\n      <td>0</td>\n      <td>263</td>\n    </tr>\n    <tr>\n      <th>9053</th>\n      <td>&gt;MED30_DANRE</td>\n      <td>MTTPPLAQFSGQQQQQTQAARDVNTASLCRIGQETVQDIVLRTMEI...</td>\n      <td>0</td>\n      <td>174</td>\n    </tr>\n    <tr>\n      <th>9054</th>\n      <td>&gt;RB87F_DROME</td>\n      <td>MAEQNDSNGNYDDGEEITEPEQLRKLFIGGLDYRTTDDGLKAHFEK...</td>\n      <td>0</td>\n      <td>385</td>\n    </tr>\n    <tr>\n      <th>9055</th>\n      <td>&gt;RS17_SPOFR</td>\n      <td>MGRVRTKTVKKAAKIIIEKYYTRLTLDFDTNKRICEEIAIIPTKPL...</td>\n      <td>0</td>\n      <td>133</td>\n    </tr>\n  </tbody>\n</table>\n<p>9056 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the dataset\nfile_path = '/kaggle/input/cobmineddb/combined_peptides.csv'\npeptide_data = pd.read_csv(file_path)\npeptide_data","metadata":{"execution":{"iopub.status.busy":"2024-09-27T09:11:10.316654Z","iopub.execute_input":"2024-09-27T09:11:10.317293Z","iopub.status.idle":"2024-09-27T09:11:10.347517Z","shell.execute_reply.started":"2024-09-27T09:11:10.317253Z","shell.execute_reply":"2024-09-27T09:11:10.346681Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"              name                                         sequence  label  \\\n0     >COC2C_CONCL    DVCDSLVGGRCIHNGCYCERDAPNGNCCNTDGCTARWWCPGTKWD      1   \n1      >NA15_ANTXA  GVSCLCDSDGPSVSGNTLSGIIWLAGCPSGWHNCKAHGPNIGWCCKK      1   \n2      >PPK6_SHELA                                   SESEVPGMWFGPRL      0   \n3       >SK1_BLAGE                                      EQFDDYGHMRF      0   \n4      >COMB_CONMA          AATCTHWALIYFKTVQLFGWHFNYQVDATYCPQFQPCMP      1   \n...            ...                                              ...    ...   \n5616           NaN                         EDDHHHHHHHHHGVGGGGGGGGGG      0   \n5617           NaN        MTTNTQYIYPIFTVRWLAVHALAVPTVFFLGSISAMQFIQR      0   \n5618           NaN                              GILDVAKTLVGKLRNVLGI      0   \n5619           NaN   MKVLSSLASAKTRYPDCQVVRRRGRVYVICKSNPRFKAVQGRKKRR      0   \n5620           NaN  MKSIDLTILSLKRKGIRTEKVLKNQNPDRLSHMTDKNAQPKSKEKEE      0   \n\n      sequence_length  \n0                  45  \n1                  47  \n2                  14  \n3                  11  \n4                  39  \n...               ...  \n5616               24  \n5617               41  \n5618               19  \n5619               46  \n5620               47  \n\n[5621 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>sequence</th>\n      <th>label</th>\n      <th>sequence_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&gt;COC2C_CONCL</td>\n      <td>DVCDSLVGGRCIHNGCYCERDAPNGNCCNTDGCTARWWCPGTKWD</td>\n      <td>1</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&gt;NA15_ANTXA</td>\n      <td>GVSCLCDSDGPSVSGNTLSGIIWLAGCPSGWHNCKAHGPNIGWCCKK</td>\n      <td>1</td>\n      <td>47</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&gt;PPK6_SHELA</td>\n      <td>SESEVPGMWFGPRL</td>\n      <td>0</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&gt;SK1_BLAGE</td>\n      <td>EQFDDYGHMRF</td>\n      <td>0</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&gt;COMB_CONMA</td>\n      <td>AATCTHWALIYFKTVQLFGWHFNYQVDATYCPQFQPCMP</td>\n      <td>1</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5616</th>\n      <td>NaN</td>\n      <td>EDDHHHHHHHHHGVGGGGGGGGGG</td>\n      <td>0</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>5617</th>\n      <td>NaN</td>\n      <td>MTTNTQYIYPIFTVRWLAVHALAVPTVFFLGSISAMQFIQR</td>\n      <td>0</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>5618</th>\n      <td>NaN</td>\n      <td>GILDVAKTLVGKLRNVLGI</td>\n      <td>0</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>5619</th>\n      <td>NaN</td>\n      <td>MKVLSSLASAKTRYPDCQVVRRRGRVYVICKSNPRFKAVQGRKKRR</td>\n      <td>0</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>5620</th>\n      <td>NaN</td>\n      <td>MKSIDLTILSLKRKGIRTEKVLKNQNPDRLSHMTDKNAQPKSKEKEE</td>\n      <td>0</td>\n      <td>47</td>\n    </tr>\n  </tbody>\n</table>\n<p>5621 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install lazypredict\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter  # Import Counter\nimport itertools\nfrom lazypredict.Supervised import LazyClassifier\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2024-09-27T09:11:12.562776Z","iopub.execute_input":"2024-09-27T09:11:12.563120Z","iopub.status.idle":"2024-09-27T09:11:29.611321Z","shell.execute_reply.started":"2024-09-27T09:11:12.563088Z","shell.execute_reply":"2024-09-27T09:11:29.610284Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Protein Prediction","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport itertools\nfrom collections import Counter\nfrom lazypredict.Supervised import LazyClassifier\n\n# Load the dataset\ndata = protein_data\n\n# Define the feature extraction functions\ndef calculate_aac(sequence):\n    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n    sequence_length = len(sequence)\n    aa_count = Counter(sequence)\n    aac = [aa_count[aa] / sequence_length for aa in amino_acids]\n    return aac\n\ndef calculate_dpc(sequence):\n    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n    dipeptides = [''.join(pair) for pair in itertools.product(amino_acids, repeat=2)]\n    dipeptide_count = Counter([sequence[i:i+2] for i in range(len(sequence)-1)])\n    sequence_length = len(sequence) - 1\n    dpc = [dipeptide_count[dp] / sequence_length for dp in dipeptides]\n    return dpc\n\n# Extract features for a given dataset with renamed columns to avoid overlap\ndef extract_features(data):\n    aac_features = data['sequence'].apply(calculate_aac)\n    dpc_features = data['sequence'].apply(calculate_dpc) \n\n    aac_df = pd.DataFrame(aac_features.tolist(), columns=[f'aac_{i}' for i in range(20)])\n    dpc_df = pd.DataFrame(dpc_features.tolist(), columns=[f'dpc_{i}' for i in range(400)])\n  \n    features = aac_df.join(dpc_df)\n    \n    return features\n\n# Extract features and save them to CSV\nfeatures = extract_features(data)\nfeatures['label'] = data['label']\nfeatures.to_csv('/kaggle/working/protein_features.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T09:11:29.613254Z","iopub.execute_input":"2024-09-27T09:11:29.614186Z","iopub.status.idle":"2024-09-27T09:11:37.692309Z","shell.execute_reply.started":"2024-09-27T09:11:29.614134Z","shell.execute_reply":"2024-09-27T09:11:37.691293Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load the features from the saved CSV\nloaded_data = pd.read_csv('/kaggle/working/protein_features.csv')\n\n# Split the data into training (60%), validation (20%), and testing (20%) sets\ntrain_data, temp_data = train_test_split(loaded_data, test_size=0.4, random_state=42)\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n\n# Separate features and labels\nX_train = train_data.drop(columns=['label'])\ny_train = train_data['label']\nX_val = val_data.drop(columns=['label'])\ny_val = val_data['label']\nX_test = test_data.drop(columns=['label'])\ny_test = test_data['label']\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate models using LazyPredict\nclf = LazyClassifier()\nmodels, predictions = clf.fit(X_train, X_val, y_train, y_val)\n\n# Display model performance\nprint(models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data,val_data,test_data \n\nimport matplotlib.pyplot as plt\nprint(\"full data\")\n# Calculate label distribution\nlabel_distribution = loaded_data['label'].value_counts()\nprint(label_distribution)\n# Calculate the percentages of each label\nlabel_percentages = (label_distribution / label_distribution.sum()) * 100\nprint(label_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data,val_data,test_data \n\nimport matplotlib.pyplot as plt\nprint(\"Training\")\n# Calculate label distribution\nlabel_distribution = train_data['label'].value_counts()\nprint(label_distribution)\n# Calculate the percentages of each label\nlabel_percentages = (label_distribution / label_distribution.sum()) * 100\nprint(label_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data,val_data,test_data \n\nimport matplotlib.pyplot as plt\nprint(\"Testing\")\n# Calculate label distribution\nlabel_distribution = test_data['label'].value_counts()\nprint(label_distribution)\n# Calculate the percentages of each label\nlabel_percentages = (label_distribution / label_distribution.sum()) * 100\nprint(label_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data,val_data,test_data \n\nimport matplotlib.pyplot as plt\nprint(\"Validation\")\n# Calculate label distribution\nlabel_distribution = val_data['label'].value_counts()\nprint(label_distribution)\n# Calculate the percentages of each label\nlabel_percentages = (label_distribution / label_distribution.sum()) * 100\nprint(label_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = models.index[0]\nbest_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc, confusion_matrix, classification_report\n\n# Define the model dictionary including AdaBoostClassifier\nmodel_dict = {\n    'LGBMClassifier': LGBMClassifier(),\n    'XGBClassifier': XGBClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'LogisticRegression': LogisticRegression(),\n    'SVC': SVC(probability=True),  # SVC needs probability=True for AUROC\n    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n}\n\n# Function to calculate additional metrics\ndef calculate_metrics(y_true, y_pred, y_prob=None):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    mcc = matthews_corrcoef(y_true, y_pred)\n    \n    sensitivity = tp / (tp + fn)  # Sensitivity (SN)\n    specificity = tn / (tn + fp)  # Specificity (SP)\n    fdr = fp / (fp + tp)  # False Discovery Rate (FDR)\n    \n    if y_prob is not None:\n        auroc = roc_auc_score(y_true, y_prob[:, 1])\n        precision, recall, _ = precision_recall_curve(y_true, y_prob[:, 1])\n        auprc = auc(recall, precision)\n    else:\n        auroc = None\n        auprc = None\n\n    return {\n        'accuracy': accuracy,\n        'f1_score': f1,\n        'mcc': mcc,\n        'sensitivity': sensitivity,\n        'specificity': specificity,\n        'fdr': fdr,\n        'auroc': auroc,\n        'auprc': auprc\n    }\n\n# Fit and evaluate each model\nbest_model_name = None\nbest_accuracy = 0\nresults = {}\n\nfor model_name, model in model_dict.items():\n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    val_predictions = model.predict(X_val)\n    test_predictions = model.predict(X_test)\n    \n    # Probability predictions for ROC and AUPRC\n    if hasattr(model, \"predict_proba\"):\n        val_prob = model.predict_proba(X_val)\n        test_prob = model.predict_proba(X_test)\n    else:\n        val_prob = None\n        test_prob = None\n    \n    # Calculate metrics for validation set\n    val_metrics = calculate_metrics(y_val, val_predictions, val_prob)\n    test_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n    \n    # Store results\n    results[model_name] = {\n        'validation_metrics': val_metrics,\n        'test_metrics': test_metrics\n    }\n    \n    # Print metrics\n    print(f\"\\n{model_name} Validation Metrics:\")\n    for metric, value in val_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    print(f\"\\n{model_name} Test Metrics:\")\n    for metric, value in test_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    # Track the best model by accuracy\n    if val_metrics['accuracy'] > best_accuracy:\n        best_accuracy = val_metrics['accuracy']\n        best_model_name = model_name\n\nprint(f\"\\nBest Model Name: {best_model_name}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n# Define base models for stacking\nbase_models = [\n    ('lgbm', LGBMClassifier()),  # LightGBM model\n    ('svc', SVC(probability=True)),  # Support Vector Classifier\n    ('xgb', XGBClassifier(tree_method='gpu_hist', gpu_id=0))  # XGBoost model with GPU\n]\n\n# Create stacking classifier\nstacking_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression(),  # Final estimator\n    cv=5  # Use 5-fold cross-validation\n)\n\n# Fit the stacking model\nstacking_model.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on validation and test sets\nval_predictions = stacking_model.predict(X_val)\ntest_predictions = stacking_model.predict(X_test)\n\n# Probability predictions for ROC and AUPRC\nif hasattr(stacking_model, \"predict_proba\"):\n    val_prob = stacking_model.predict_proba(X_val)\n    test_prob = stacking_model.predict_proba(X_test)\nelse:\n    val_prob = None\n    test_prob = None\n\n# Calculate metrics for validation set\nval_metrics = calculate_metrics(y_val, val_predictions, val_prob)\ntest_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n\n# Print stacking model metrics\nprint(\"\\nStacking Model Validation Metrics:\")\nfor metric, value in val_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\nprint(\"\\nStacking Model Test Metrics:\")\nfor metric, value in test_metrics.items():\n    print(f\"{metric}:Â {value:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform cross-validation for stacking model using the training set\nn_folds = 5  # You can adjust this number\ncv_scores = cross_val_score(stacking_model, X_train, y_train, cv=n_folds, scoring='accuracy')\n\n# Calculate average cross-validation score\naverage_cv_score = cv_scores.mean()\nstd_cv_score = cv_scores.std()\n\n# Print cross-validation results\nprint(\"\\nCross-Validation Scores for Stacking Model:\")\nprint(f\"Scores: {cv_scores}\")\nprint(f\"Average Score: {average_cv_score:.4f} Â± {std_cv_score:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Peptide prediction","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport itertools\nfrom collections import Counter\nfrom lazypredict.Supervised import LazyClassifier\n\n# Load the dataset\ndata = peptide_data\n\n# Define the feature extraction functions\ndef calculate_aac(sequence):\n    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n    sequence_length = len(sequence)\n    aa_count = Counter(sequence)\n    aac = [aa_count[aa] / sequence_length for aa in amino_acids]\n    return aac\n\ndef calculate_dpc(sequence):\n    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n    dipeptides = [''.join(pair) for pair in itertools.product(amino_acids, repeat=2)]\n    dipeptide_count = Counter([sequence[i:i+2] for i in range(len(sequence)-1)])\n    sequence_length = len(sequence) - 1\n    dpc = [dipeptide_count[dp] / sequence_length for dp in dipeptides]\n    return dpc\n\n# Extract features for a given dataset with renamed columns to avoid overlap\ndef extract_features(data):\n    aac_features = data['sequence'].apply(calculate_aac)\n    dpc_features = data['sequence'].apply(calculate_dpc)\n\n    aac_df = pd.DataFrame(aac_features.tolist(), columns=[f'aac_{i}' for i in range(20)])\n    dpc_df = pd.DataFrame(dpc_features.tolist(), columns=[f'dpc_{i}' for i in range(400)])\n\n    features = aac_df.join(dpc_df)\n    \n    return features\n\n# Extract features and save them to CSV\nfeatures = extract_features(data)\nfeatures['label'] = data['label']\nfeatures.to_csv('/kaggle/working/peptide_features.csv', index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-27T09:12:30.675880Z","iopub.execute_input":"2024-09-27T09:12:30.676252Z","iopub.status.idle":"2024-09-27T09:12:34.108142Z","shell.execute_reply.started":"2024-09-27T09:12:30.676217Z","shell.execute_reply":"2024-09-27T09:12:34.107360Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load the features from the saved CSV\nloaded_data = pd.read_csv('/kaggle/working/peptide_features.csv')\n\n# Split the data into training (60%), validation (20%), and testing (20%) sets\ntrain_data, temp_data = train_test_split(loaded_data, test_size=0.4, random_state=42)\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n\n# Separate features and labels\nX_train = train_data.drop(columns=['label'])\ny_train = train_data['label']\nX_val = val_data.drop(columns=['label'])\ny_val = val_data['label']\nX_test = test_data.drop(columns=['label'])\ny_test = test_data['label']\n\n# Evaluate models using LazyPredict\nclf = LazyClassifier()\nmodels, predictions = clf.fit(X_train, X_val, y_train, y_val)\n\n# Display model performance\nprint(models)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = models.index[0]\nbest_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data,val_data,test_data \n\nimport matplotlib.pyplot as plt\nprint(\"full\")\n# Calculate label distribution\nlabel_distribution = loaded_data['label'].value_counts()\nprint(label_distribution)\n# Calculate the percentages of each label\nlabel_percentages = (label_distribution / label_distribution.sum()) * 100\nprint(label_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data,val_data,test_data \n\nimport matplotlib.pyplot as plt\nprint(\"Training\")\n# Calculate label distribution\nlabel_distribution = train_data['label'].value_counts()\nprint(label_distribution)\n# Calculate the percentages of each label\nlabel_percentages = (label_distribution / label_distribution.sum()) * 100\nprint(label_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data,val_data,test_data \n\nimport matplotlib.pyplot as plt\nprint(\"Test\")\n# Calculate label distribution\nlabel_distribution = test_data['label'].value_counts()\nprint(label_distribution)\n# Calculate the percentages of each label\nlabel_percentages = (label_distribution / label_distribution.sum()) * 100\nprint(label_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_data,val_data,test_data \n\nimport matplotlib.pyplot as plt\nprint(\"Validation\")\n# Calculate label distribution\nlabel_distribution = val_data['label'].value_counts()\nprint(label_distribution)\n# Calculate the percentages of each label\nlabel_percentages = (label_distribution / label_distribution.sum()) * 100\nprint(label_percentages)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc, confusion_matrix, classification_report\n\n# Define the model dictionary including AdaBoostClassifier\nmodel_dict = {\n    'LGBMClassifier': LGBMClassifier(),\n    'XGBClassifier': XGBClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'LogisticRegression': LogisticRegression(),\n    'SVC': SVC(probability=True),  # SVC needs probability=True for AUROC\n    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n}\n\n# Function to calculate additional metrics\ndef calculate_metrics(y_true, y_pred, y_prob=None):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    mcc = matthews_corrcoef(y_true, y_pred)\n    \n    sensitivity = tp / (tp + fn)  # Sensitivity (SN)\n    specificity = tn / (tn + fp)  # Specificity (SP)\n    fdr = fp / (fp + tp)  # False Discovery Rate (FDR)\n    \n    if y_prob is not None:\n        auroc = roc_auc_score(y_true, y_prob[:, 1])\n        precision, recall, _ = precision_recall_curve(y_true, y_prob[:, 1])\n        auprc = auc(recall, precision)\n    else:\n        auroc = None\n        auprc = None\n\n    return {\n        'accuracy': accuracy,\n        'f1_score': f1,\n        'mcc': mcc,\n        'sensitivity': sensitivity,\n        'specificity': specificity,\n        'fdr': fdr,\n        'auroc': auroc,\n        'auprc': auprc\n    }\n\n# Fit and evaluate each model\nbest_model_name = None\nbest_accuracy = 0\nresults = {}\n\nfor model_name, model in model_dict.items():\n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    val_predictions = model.predict(X_val)\n    test_predictions = model.predict(X_test)\n    \n    # Probability predictions for ROC and AUPRC\n    if hasattr(model, \"predict_proba\"):\n        val_prob = model.predict_proba(X_val)\n        test_prob = model.predict_proba(X_test)\n    else:\n        val_prob = None\n        test_prob = None\n    \n    # Calculate metrics for validation set\n    val_metrics = calculate_metrics(y_val, val_predictions, val_prob)\n    test_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n    \n    # Store results\n    results[model_name] = {\n        'validation_metrics': val_metrics,\n        'test_metrics': test_metrics\n    }\n    \n    # Print metrics\n    print(f\"\\n{model_name} Validation Metrics:\")\n    for metric, value in val_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    print(f\"\\n{model_name} Test Metrics:\")\n    for metric, value in test_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \n    # Track the best model by accuracy\n    if val_metrics['accuracy'] > best_accuracy:\n        best_accuracy = val_metrics['accuracy']\n        best_model_name = model_name\n\nprint(f\"\\nBest Model Name: {best_model_name}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Define base models for stacking\nbase_models = [\n    ('lgbm', LGBMClassifier()),  # LightGBM model\n    ('extra_trees', ExtraTreesClassifier()),  # Extra Trees Classifier\n    ('xgb', XGBClassifier(tree_method='gpu_hist', gpu_id=0))  # XGBoost model with GPU\n]\n\n# Create stacking classifier\nstacking_model = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression(),  # You can choose any classifier here\n    cv=5  # Use 5-fold cross-validation\n)\n\n# Fit the stacking model\nstacking_model.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on validation and test sets\nval_predictions = stacking_model.predict(X_val)\ntest_predictions = stacking_model.predict(X_test)\n\n# Probability predictions for ROC and AUPRC\nif hasattr(stacking_model, \"predict_proba\"):\n    val_prob = stacking_model.predict_proba(X_val)\n    test_prob = stacking_model.predict_proba(X_test)\nelse:\n    val_prob = None\n    test_prob = None\n\n# Calculate metrics for validation set\nval_metrics = calculate_metrics(y_val, val_predictions, val_prob)\ntest_metrics = calculate_metrics(y_test, test_predictions, test_prob)\n\n# Print stacking model metrics\nprint(\"\\nStacking Model Validation Metrics:\")\nfor metric, value in val_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\nprint(\"\\nStacking Model Test Metrics:\")\nfor metric, value in test_metrics.items():\n    print(f\"{metric}:Â {value:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perform cross-validation for stacking model using the training set\nn_folds = 5  # You can adjust this number\ncv_scores = cross_val_score(stacking_model, X_train, y_train, cv=n_folds, scoring='accuracy')\n\n# Calculate average cross-validation score\naverage_cv_score = cv_scores.mean()\nstd_cv_score = cv_scores.std()\n\n# Print cross-validation results\nprint(\"\\nCross-Validation Scores for Stacking Model:\")\nprint(f\"Scores: {cv_scores}\")\nprint(f\"Average Score: {average_cv_score:.4f} Â± {std_cv_score:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}