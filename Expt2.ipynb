{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8732319,"sourceType":"datasetVersion","datasetId":5241556},{"sourceId":8732333,"sourceType":"datasetVersion","datasetId":5241568},{"sourceId":9088340,"sourceType":"datasetVersion","datasetId":5483951}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers pandas scikit-learn torch lazypredict\n","metadata":{"_uuid":"9fa324ca-9f63-4a0e-9b40-c263d9b4d023","_cell_guid":"976fc78c-7561-46a4-89a9-301a83a414ac","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## peptides","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertForMaskedLM, BertTokenizer, BertModel\n\n# Load the CSV file\n# file_path = 'path/to/your/combined_peptides.csv'\ndata = pd.read_csv('/kaggle/input/new-combined-data/combined_peptides.csv')\n\n# Load the pretrained ProtBERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert', do_lower_case=False)\nmodel = BertModel.from_pretrained('Rostlab/prot_bert')\nmodel.to('cuda')\n\ndef add_spaces(s):\n    return ' '.join(s)\n\n# Prepare the sequence data\nsequences = data['sequence'].tolist()\nsequences = [add_spaces(s) for s in sequences]\nlabels = data['label'].tolist()\n\n# Tokenize sequences\ndef tokenize_sequences(sequences, tokenizer, max_length=512):\n    tokenized_sequences = []\n    for seq in sequences:\n        # Tokenize and encode the sequence\n        inputs = tokenizer(seq, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n        tokenized_sequences.append(inputs)\n    return tokenized_sequences\n\ntokenized_sequences = tokenize_sequences(sequences, tokenizer)\n\n# Extract features\n# Extract features\ndef extract_features(tokenized_sequences, model):\n    model.eval()\n    all_embeddings = []\n    with torch.no_grad():\n        for tokenized_sequence in tokenized_sequences:\n            # Move tokenized sequence to CUDA\n            tokenized_sequence = {key: val.to('cuda') for key, val in tokenized_sequence.items()}\n            # Get model outputs\n            outputs = model(**tokenized_sequence)\n            # Compute mean pooling on the token embeddings\n            sequence_embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n            all_embeddings.append(sequence_embeddings)\n    return all_embeddings\n\nsequence_embeddings = extract_features(tokenized_sequences, model)\n\n# Convert the embeddings to a DataFrame\nembedding_df = pd.DataFrame(sequence_embeddings)\nembedding_df['label'] = labels\n\nembedding_df.to_csv('/kaggle/working/embeddings.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load the embeddings\nembedding_df = pd.read_csv('/kaggle/working/embeddings.csv')\n\n# Split the data into features and labels\nX = embedding_df.drop(columns=['label'])\ny = embedding_df['label']\n\n# First split into training and temporary (test + validation) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Second split to divide temporary set into validation and test sets\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Save the split datasets\nX_train.to_csv('/kaggle/working/X_train.csv', index=False)\nX_test.to_csv('/kaggle/working/X_test.csv', index=False)\ny_train.to_csv('/kaggle/working/y_train.csv', index=False)\ny_test.to_csv('/kaggle/working/y_test.csv', index=False)\nX_val.to_csv('/kaggle/working/X_val.csv', index=False)\ny_val.to_csv('/kaggle/working/y_val.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lazypredict.Supervised import LazyClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the split datasets\nX_train = pd.read_csv('/kaggle/working/X_train.csv')\nX_test = pd.read_csv('/kaggle/working/X_test.csv')\nX_val = pd.read_csv('/kaggle/working/X_val.csv')\ny_train = pd.read_csv('/kaggle/working/y_train.csv').values.ravel()\ny_test = pd.read_csv('/kaggle/working/y_test.csv').values.ravel()\ny_val = pd.read_csv('/kaggle/working/y_val.csv').values.ravel()\n\n# Initialize LazyClassifier\nclf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n\n# Train and test the models\nmodels, predictions = clf.fit(X_train, X_val, y_train, y_val)\n\n# Display the results\nprint(models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n# Define the model dictionary including AdaBoostClassifier\nmodel_dict = {\n    'LGBMClassifier': LGBMClassifier(),\n    'XGBClassifier': XGBClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'LogisticRegression': LogisticRegression(),\n    'SVC': SVC(),\n    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n}\n\n# Fit and evaluate each model\nbest_model_name = None\nbest_accuracy = 0\n\nfor model_name, model in model_dict.items():\n    model.fit(X_train, y_train)\n    val_predictions = model.predict(X_val)\n    test_predictions = model.predict(X_test)\n    val_accuracy = accuracy_score(y_val, val_predictions)\n    test_accuracy = accuracy_score(y_test, test_predictions)\n    print(f\"{model_name} validation accuracy: {val_accuracy:.4f}, test accuracy: {test_accuracy:.4f}\")\n\n    if val_accuracy > best_accuracy:\n        best_accuracy = val_accuracy\n        best_model_name = model_name\n\nprint(f\"\\nBest Model Name: {best_model_name}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## proteins","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertForMaskedLM, BertTokenizer, BertModel\n\n# Load the CSV file\n# file_path = 'path/to/your/combined_peptides.csv'\ndata = pd.read_csv('/kaggle/input/new-combined-data/combined_protein.csv')\n\n# Load the pretrained ProtBERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert', do_lower_case=False)\nmodel = BertModel.from_pretrained('Rostlab/prot_bert')\nmodel.to('cuda')\n\ndef add_spaces(s):\n    return ' '.join(s)\n\n# Prepare the sequence data\nsequences = data['sequence'].tolist()\nsequences = [add_spaces(s) for s in sequences]\nlabels = data['label'].tolist()\n\n# Tokenize sequences\ndef tokenize_sequences(sequences, tokenizer, max_length=512):\n    tokenized_sequences = []\n    for seq in sequences:\n        # Tokenize and encode the sequence\n        inputs = tokenizer(seq, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n        tokenized_sequences.append(inputs)\n    return tokenized_sequences\n\ntokenized_sequences = tokenize_sequences(sequences, tokenizer)\n\n# Extract features\n# Extract features\ndef extract_features(tokenized_sequences, model):\n    model.eval()\n    all_embeddings = []\n    with torch.no_grad():\n        for tokenized_sequence in tokenized_sequences:\n            # Move tokenized sequence to CUDA\n            tokenized_sequence = {key: val.to('cuda') for key, val in tokenized_sequence.items()}\n            # Get model outputs\n            outputs = model(**tokenized_sequence)\n            # Compute mean pooling on the token embeddings\n            sequence_embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n            all_embeddings.append(sequence_embeddings)\n    return all_embeddings\n\nsequence_embeddings = extract_features(tokenized_sequences, model)\n\n# Convert the embeddings to a DataFrame\nembedding_df = pd.DataFrame(sequence_embeddings)\nembedding_df['label'] = labels\n\nembedding_df.to_csv('/kaggle/working/embeddings_pro.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load the embeddings\nembedding_df = pd.read_csv('/kaggle/working/embeddings_pro.csv')\n\n# Split the data into features and labels\nX = embedding_df.drop(columns=['label'])\ny = embedding_df['label']\n\n# First split into training and temporary (test + validation) sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Second split to divide temporary set into validation and test sets\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Save the split datasets\nX_train.to_csv('/kaggle/working/X_train_pro.csv', index=False)\nX_test.to_csv('/kaggle/working/X_test_pro.csv', index=False)\ny_train.to_csv('/kaggle/working/y_train_pro.csv', index=False)\ny_test.to_csv('/kaggle/working/y_test_pro.csv', index=False)\nX_val.to_csv('/kaggle/working/X_val_pro.csv', index=False)\ny_val.to_csv('/kaggle/working/y_val_pro.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lazypredict.Supervised import LazyClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the split datasets\nX_train = pd.read_csv('/kaggle/working/X_train_pro.csv')\nX_test = pd.read_csv('/kaggle/working/X_test_pro.csv')\nX_val = pd.read_csv('/kaggle/working/X_val_pro.csv')\ny_train = pd.read_csv('/kaggle/working/y_train_pro.csv').values.ravel()\ny_test = pd.read_csv('/kaggle/working/y_test_pro.csv').values.ravel()\ny_val = pd.read_csv('/kaggle/working/y_val_pro.csv').values.ravel()\n\n# Initialize LazyClassifier\nclf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n\n# Train and test the models\nmodels, predictions = clf.fit(X_train, X_val, y_train, y_val)\n\n# Display the results\nprint(models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n# Define the model dictionary including AdaBoostClassifier\nmodel_dict = {\n    'LGBMClassifier': LGBMClassifier(),\n    'XGBClassifier': XGBClassifier(),\n    'RandomForestClassifier': RandomForestClassifier(),\n    'AdaBoostClassifier': AdaBoostClassifier(),\n    'LogisticRegression': LogisticRegression(),\n    'SVC': SVC(),\n    'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n}\n\n# Fit and evaluate each model\nbest_model_name = None\nbest_accuracy = 0\n\nfor model_name, model in model_dict.items():\n    model.fit(X_train, y_train)\n    val_predictions = model.predict(X_val)\n    test_predictions = model.predict(X_test)\n    val_accuracy = accuracy_score(y_val, val_predictions)\n    test_accuracy = accuracy_score(y_test, test_predictions)\n    print(f\"{model_name} validation accuracy: {val_accuracy:.4f}, test accuracy: {test_accuracy:.4f}\")\n\n    if val_accuracy > best_accuracy:\n        best_accuracy = val_accuracy\n        best_model_name = model_name\n\nprint(f\"\\nBest Model Name: {best_model_name}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}